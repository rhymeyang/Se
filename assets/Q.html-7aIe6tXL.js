import{_ as i,c as t,a as s,o as n}from"./app-koRvTuuZ.js";const o={};function r(a,e){return n(),t("div",null,e[0]||(e[0]=[s('<ul><li>https://www.itexams.com/exam/CISA</li></ul><h2 id="chapter-4-it-life-cycle-management" tabindex="-1"><a class="header-anchor" href="#chapter-4-it-life-cycle-management"><span>CHAPTER 4 IT Life Cycle Management</span></a></h2><ol><li><p>What testing activities should developers perform during the development phase?</p><ul><li><strong>A.</strong> Security testing</li><li><strong>B.</strong> Integration testing</li><li><strong>C.</strong> Unit testing</li><li><strong>D.</strong> Developers should not perform any testing</li><li>Answer: <strong>C.</strong> During the development phase, developers should perform only unit testing to verify that the individual sections of code they have written are performing properly.</li><li><strong>C. Unit testing</strong></li><li>During the development phase, unit testing is the primary testing activity performed by developers. This involves testing individual components or modules of the code to ensure they function as intended in isolation. Unit testing helps identify bugs early in the development process, reducing the cost and effort required to fix issues later.</li><li>Other Options: <ul><li>A. Security testing: While important, this is typically conducted by specialized teams during the testing or deployment phases, not primarily by developers during development.</li><li>B. Integration testing: This is usually performed after unit testing to verify the interaction between modules or components.</li><li>D. Developers should not perform any testing: This is incorrect, as developers are responsible for performing unit testing during the development phase.</li></ul></li></ul></li><li><p>The purpose of <strong>function point analysis (FPA)</strong> is to</p><ul><li><strong>A.</strong> Estimate the effort required to develop a software program.</li><li><strong>B.</strong> Identify risks in a software program.</li><li><strong>C.</strong> Estimate task dependencies in a project plan.</li><li><strong>D.</strong> Inventory inputs and outputs in a software program.</li><li>Answer: <strong>A.</strong> Function point analysis (FPA) is used to estimate the effort required to develop a software program.</li><li><strong>A. Estimate the effort required to develop a software program.</strong></li><li>Explanation: <ul><li>Function Point Analysis (FPA) is a standardized method used to measure the size and complexity of software by quantifying its functional components, such as inputs, outputs, user interactions, files, and interfaces. The primary purpose of FPA is to estimate the effort, cost, and time required for software development or maintenance based on these measurements.</li></ul></li><li>Other Options: <ul><li>B. Identify risks in a software program: FPA is not designed for risk assessment.</li><li>C. Estimate task dependencies in a project plan: This is typically done through techniques like critical path analysis or project management tools, not FPA.</li><li>D. Inventory inputs and outputs in a software program: While FPA involves identifying inputs and outputs, its purpose goes beyond inventorying—it focuses on estimating development effort.</li></ul></li></ul></li><li><p>A project manager needs to identify the tasks that are responsible for project delays. What approach should the project manager use?</p><ul><li><strong>A.</strong> Function point analysis</li><li><strong>B.</strong> Gantt analysis</li><li><strong>C.</strong> Project evaluation and review technique</li><li><strong>D.</strong> Critical path methodology</li><li>Answer: <strong>D.</strong> Critical path methodology helps a project manager determine which activities are on a project’s “critical path.”</li><li><strong>D. Critical path methodology</strong></li><li>Explanation: <ul><li>The Critical Path Methodology (CPM) is used to identify the sequence of tasks that determine the minimum project duration. If tasks on the critical path are delayed, the entire project is delayed. By analyzing the critical path, the project manager can identify which tasks are responsible for project delays and take corrective action.</li></ul></li><li>Other Options: <ul><li>A. Function point analysis: This is used for estimating the effort required for software development, not for managing project timelines or delays.</li><li>B. Gantt analysis: While Gantt charts help visualize task schedules, they do not specifically highlight the tasks responsible for project delays.</li><li>C. Project evaluation and review technique (PERT): PERT is used for estimating project timelines with uncertainty but does not directly focus on identifying tasks causing delays.</li></ul></li></ul></li><li><p>A software developer has informed the project manager that a portion of the application development is going to take five additional days to complete. The project manager should</p><ul><li><strong>A.</strong> Inform the other project participants of the schedule change.</li><li><strong>B.</strong> Change the project schedule to reflect the new completion time.</li><li><strong>C.</strong> Create a project change request.</li><li><strong>D.</strong> Adjust the resource budget to account for the schedule change.</li><li>Answer: <strong>C.</strong> When any significant change needs to occur in a project plan, a project change request should be created to document the reason for the change.</li><li><strong>C. Create a project change request.</strong></li><li>Explanation: <ul><li>When there is a significant change in the project, such as a delay, the project manager should follow the formal change management process. This involves creating a project change request to document the impact of the delay, assess its implications on the timeline, budget, and resources, and seek approval from stakeholders if necessary. This ensures transparency and proper tracking of changes.</li></ul></li><li>Other Options: <ul><li>A. Inform the other project participants of the schedule change: While important, this step should occur only after the change request is reviewed and approved.</li><li>B. Change the project schedule to reflect the new completion time: The project manager should not unilaterally change the schedule without following the formal change management process.</li><li>D. Adjust the resource budget to account for the schedule change: This step may be necessary but only after the change request is approved and its impact assessed.</li></ul></li></ul></li><li><p>The phases and their order in the systems development life cycle are</p><ul><li><strong>A.</strong> Requirements definition, feasibility study, design, development, testing, implementation, post-implementation</li><li><strong>B.</strong> Feasibility study, requirements definition, design, development, testing, implementation, post-implementation</li><li><strong>C.</strong> Feasibility study, requirements definition, design, development, testing, implementation</li><li><strong>D.</strong> Requirements definition, feasibility study, development, testing, implementation, post-implementation</li><li>Answer: <strong>B.</strong> The phases of the systems development life cycle are feasibility study, requirements definition, design, development, testing, implementation, and post-implementation.</li><li><strong>B. Feasibility study, requirements definition, design, development, testing, implementation, post-implementation</strong></li><li>Explanation: <ul><li><strong>The Systems Development Life Cycle (SDLC)</strong> is a structured approach to software and system development. The phases in order are as follows:</li><li><strong>Feasibility Study:</strong> Determine if the project is viable and worth pursuing.</li><li><strong>Requirements Definition:</strong> Identify what the system should do, capturing functional and non-functional requirements.</li><li><strong>Design:</strong> Create detailed system and technical specifications.</li><li><strong>Development:</strong> Write and build the actual software or system.</li><li><strong>Testing:</strong> Validate that the system meets requirements and works as expected.</li><li><strong>Implementation:</strong> Deploy the system into a live environment.</li><li><strong>Post-Implementation:</strong> Monitor and maintain the system after deployment.</li></ul></li></ul></li><li><p>What personnel should be involved in the requirements phase of a software development project?</p><ul><li><strong>A.</strong> Systems administrators, network administrators, and software developers</li><li><strong>B.</strong> Developers, analysts, architects, and users</li><li><strong>C.</strong> Security, privacy, and legal analysts</li><li><strong>D.</strong> Representatives from each software vendor</li><li>Answer: <strong>B.</strong> Requirements need to be developed by several parties, including developers, analysts, architects, and users.</li><li>B. Developers, analysts, architects, and users</li><li>Explanation: <ul><li>The requirements phase of a software development project is crucial for gathering and documenting what the system needs to accomplish. It requires input from multiple stakeholders to ensure all perspectives are considered:</li><li><strong>Developers</strong>: Provide technical feasibility insights.</li><li><strong>Analysts</strong>: Bridge the gap between business needs and technical specifications.</li><li><strong>Architects</strong>: Define the overall structure and ensure alignment with system goals.</li><li><strong>Users</strong>: Offer insights into the functional and usability requirements, as they are the end-users of the system.</li></ul></li><li>This combination ensures a comprehensive understanding of what the system needs to achieve.</li><li>Other Options: <ul><li>A. Systems administrators, network administrators, and software developers: These roles are more relevant during the implementation and deployment phases, not the requirements phase.</li><li>C. Security, privacy, and legal analysts: While important, these roles focus on compliance and are typically involved in specific stages, such as design or post-implementation.</li><li>D. Representatives from each software vendor: This applies to vendor selection or procurement processes, not gathering requirements for a specific project.</li></ul></li></ul></li><li><p>The primary source for test plans in a software development project is</p><ul><li><strong>A.</strong> Requirements</li><li><strong>B.</strong> Developers</li><li><strong>C.</strong> End users</li><li><strong>D.</strong> Vendors</li><li>Answer: <strong>A.</strong> The requirements that are developed for a project should be the primary source for detailed tests.</li><li>A. Requirements</li><li>Explanation: <ul><li>Test plans are primarily derived from the requirements of the software development project. Requirements specify what the system is expected to do (functional requirements) and how it should perform (non-functional requirements). These specifications serve as the foundation for designing test cases to ensure the system meets its intended objectives.</li><li>By basing test plans on requirements, the project team ensures: <ul><li>Comprehensive coverage of system functionalities.</li><li>Alignment of testing activities with business goals.</li><li>Detection of deviations from expected outcomes.</li></ul></li></ul></li><li>Other Options: <ul><li>B. Developers: While developers provide technical insights, they are not the primary source for test plans.</li><li>C. End users: End users offer feedback on usability but are not the main source for formal test plans.</li><li>D. Vendors: Vendors may provide testing tools or guidelines but do not define the specific test plans for a project.</li></ul></li></ul></li><li><p>The primary purpose of a change management process is to</p><ul><li><strong>A.</strong> Record changes made to systems and infrastructure.</li><li><strong>B.</strong> Review and approve proposed changes to systems and infrastructure.</li><li><strong>C.</strong> Review and approve changes to a project schedule.</li><li><strong>D.</strong> Review and approve changes to application source code.</li><li>Answer: <strong>B.</strong> The main purpose of change management is to review and approve proposed changes to systems and infrastructure. This helps to reduce the risk of unintended events and unplanned downtime.</li><li><strong>B. Review and approve proposed changes to systems and infrastructure.</strong></li><li>Explanation: <ul><li>The primary purpose of a change management process is to ensure that all changes to systems, infrastructure, or processes are properly reviewed, approved, and documented before implementation. This minimizes the risks associated with unauthorized or unplanned changes, such as system downtime, security vulnerabilities, or operational disruptions.</li></ul></li><li>The process typically includes: <ul><li>Submitting a change request.</li><li>Reviewing the request for potential impacts.</li><li>Approving or rejecting the request based on evaluation.</li><li>Implementing the approved change with appropriate tracking.</li></ul></li><li>Other Options: <ul><li>A. Record changes made to systems and infrastructure: While change management includes documentation, its primary goal is broader and focuses on reviewing and approving changes beforehand.</li><li>C. Review and approve changes to a project schedule: This is handled under project management, not general change management.</li><li>D. Review and approve changes to application source code: Code changes are managed through version control systems and code review processes, which are components of development, not overall change management.</li></ul></li></ul></li><li><p>What is the purpose of a capability maturity model?</p><ul><li><strong>A.</strong> To assess the experience of software developers</li><li><strong>B.</strong> To assess the experience of project managers</li><li><strong>C.</strong> To assess the integrity of application software</li><li><strong>D.</strong> To assess the maturity of business processes</li><li>Answer: <strong>D.</strong> A capability maturity model helps an organization to assess the maturity of its business processes, which is an important first step to any large-scale process improvement efforts.</li><li><strong>D. To assess the maturity of business processes</strong></li><li>Explanation: <ul><li>The Capability Maturity Model (CMM) is a framework used to assess and improve the maturity of business processes, particularly in software development and project management. It evaluates processes based on their effectiveness, efficiency, and consistency. The model defines maturity levels that help organizations identify their current state and develop a roadmap for process improvement.</li><li>The Five Maturity Levels: <ul><li>Initial (Level 1): Processes are ad hoc and chaotic.</li><li>Repeatable (Level 2): Processes are established but not standardized.</li><li>Defined (Level 3): Processes are standardized and documented.</li><li>Managed (Level 4): Processes are measured and controlled.</li><li>Optimizing (Level 5): Continuous process improvement is implemented.</li></ul></li></ul></li><li>Other Options: <ul><li>A. To assess the experience of software developers: CMM focuses on processes, not individual skills.</li><li>B. To assess the experience of project managers: Similar to A, this is not the focus of CMM.</li><li>C. To assess the integrity of application software: Integrity is addressed through specific controls or frameworks, not CMM.</li></ul></li></ul></li><li><p>The purpose of input validation checking is to</p><ul><li><strong>A.</strong> Ensure that input values are within acceptable ranges.</li><li><strong>B.</strong> Ensure that input data contains the correct type of characters.</li><li><strong>C.</strong> Ensure that input data is free of hostile or harmful content.</li><li><strong>D.</strong> Ensure all of the above.</li><li>Answer: <strong>D.</strong> Input validation checking is used to ensure that input values are within established ranges, of the correct character types, and free of harmful content.</li><li><strong>D. Ensure all of the above.</strong></li><li>Explanation: <ul><li>Input validation checking serves multiple purposes to ensure the integrity, security, and correctness of input data. These purposes include:</li><li>Ensuring input values are within acceptable ranges: This prevents errors and ensures data accuracy (e.g., a user age field should only accept values between 0 and 120).</li><li>Ensuring input data contains the correct type of characters: This prevents format-related errors (e.g., numeric fields should not contain letters).</li><li>Ensuring input data is free of hostile or harmful content: This protects against malicious attacks like SQL injection or cross-site scripting (XSS).</li></ul></li></ul></li><li><p>An organization is considering the acquisition of enterprise software that will be hosted by a cloud services provider. What additional requirements need to be considered for the cloud environment?</p><ul><li><strong>A.</strong> Logging</li><li><strong>B.</strong> Access control</li><li><strong>C.</strong> Data segregation</li><li><strong>D.</strong> Performance</li><li>Answer: <strong>C.</strong> In addition to business, functional, security, and privacy requirements, an organization considering cloud-based services needs to understand how the cloud services provider segregates the organization’s data from that of its other customers.</li><li><strong>C. Data segregation</strong></li><li>Explanation: <ul><li>When enterprise software is hosted by a cloud services provider, data segregation becomes a critical requirement. Cloud environments often host data from multiple clients, so ensuring that an organization&#39;s data is properly isolated from other clients&#39; data is essential for maintaining data confidentiality and security. Data segregation ensures compliance with legal and regulatory requirements and protects against data breaches or unauthorized access.</li></ul></li><li>Other Options: <ul><li>A. Logging: Logging is important for monitoring and auditing purposes, but it is not unique to the cloud environment—it applies to both on-premises and cloud solutions.</li><li>B. Access control: While access control is vital, it is not specific to the cloud; it is a general requirement for securing any system.</li><li>D. Performance: Performance is a consideration for all environments but does not specifically address the unique challenges of a cloud-hosted solution.</li></ul></li></ul></li><li><p>System operators have to make an emergency change in order to keep an application server running. To satisfy change management requirements, the systems operators should</p><ul><li><strong>A.</strong> Document the steps taken.</li><li><strong>B.</strong> Fill out an emergency change request form.</li><li><strong>C.</strong> Seek approval from management before making the change.</li><li><strong>D.</strong> Do all of the above.</li><li>Answer: <strong>D.</strong> When making an emergency change, personnel should first seek management approval, document the details of the change, and initiate an emergency change management procedure.</li><li><strong>D. Do all of the above.</strong></li><li>Explanation: <ul><li>In an emergency change scenario, it is critical to follow change management requirements even under time constraints. The following steps are necessary to ensure the emergency change is appropriately handled:</li><li><strong>Document the steps taken:</strong> This ensures there is a record of what was done, which is essential for accountability, troubleshooting, and audit purposes.</li><li><strong>Fill out an emergency change request form:</strong> This formalizes the change and aligns it with the organization&#39;s change management policies.</li><li><strong>Seek approval from management before making the change:</strong> Whenever possible, even in emergencies, obtaining management approval ensures the change is authorized and complies with governance processes.</li><li>Following all these steps ensures that emergency changes are controlled, traceable, and compliant with organizational policies.</li></ul></li></ul></li><li><p>A global organization is planning the migration of a business process to a new application. What cutover methods can be considered?</p><ul><li><strong>A.</strong> Parallel, geographic, module by module, or all at once</li><li><strong>B.</strong> Parallel, geographic, or module by module</li><li><strong>C.</strong> Parallel, module by module, or all at once</li><li><strong>D.</strong> Parallel, geographic, or all at once</li><li>Answer: <strong>A.</strong> The migration to a new application can be done in several ways: parallel (running old and new systems side by side); geographic (migrating users in each geographic region separately); module by module (migrating individual modules of the application); or migrate all users, locations, and modules at the same time.</li><li><strong>A. Parallel, geographic, module by module, or all at once</strong></li><li>Explanation: <ul><li>When migrating a business process to a new application, several cutover methods can be considered, depending on the organization&#39;s needs, risks, and resources: <ul><li><strong>Parallel</strong>: Running the old and new systems simultaneously for a period of time to ensure the new system works correctly before decommissioning the old one.</li><li><strong>Geographic</strong>: Migrating the process region by region or location by location, useful for global organizations.</li><li><strong>Module by Module</strong>: Transitioning specific parts or modules of the process incrementally to manage complexity and risk.</li><li><strong>All at Once (Big Bang):</strong> Switching entirely to the new system in one go, which is faster but riskier if problems arise.</li></ul></li><li>This variety of methods provides flexibility to choose the best approach based on the organization&#39;s operational, technical, and risk considerations.</li><li>Other Options: <ul><li>B, C, and D: These omit one or more viable cutover methods, such as geographic or module by module, making them incomplete.</li></ul></li></ul></li></ul></li><li><p>The purpose of developing risk tiers in third-party management is to</p><ul><li><strong>A.</strong> Determine whether to perform penetration tests.</li><li><strong>B.</strong> Satisfy regulatory requirements.</li><li><strong>C.</strong> Determine the appropriate level of due diligence.</li><li><strong>D.</strong> Determine data classification requirements.</li><li>Answer: <strong>C.</strong> Developing risk tiers in third-party management helps an organization determine the level of due diligence for third parties at each risk tier. Because the level of risk varies, some third parties warrant extensive due diligence, while a lighter touch is warranted for low-risk parties.</li><li><strong>C. Determine the appropriate level of due diligence.</strong></li><li>Explanation: <ul><li>Risk tiers in third-party management are developed to categorize vendors or third parties based on the level of risk they pose to the organization. This helps in determining the appropriate level of due diligence required for each tier. For instance, vendors handling sensitive or critical data may require more rigorous assessment and oversight than those with minimal access to the organization&#39;s systems or data.</li></ul></li><li>Why the Other Options Are Incorrect: <ul><li>A. Determine whether to perform penetration tests: While risk tiers may indirectly inform this, penetration testing is not the primary purpose of risk tiers.</li><li>B. Satisfy regulatory requirements: Risk tiers can help in meeting regulatory requirements, but this is not their primary purpose.</li><li>D. Determine data classification requirements: Data classification is a separate activity and not the main objective of risk tiering in third-party management.</li></ul></li></ul></li><li><p>The reason that functional requirements need to be measurable is</p><ul><li><strong>A.</strong> Developers need to know how to test functional requirements</li><li><strong>B.</strong> Functional tests are derived directly from functional requirements</li><li><strong>C.</strong> To verify correct system operation</li><li><strong>D.</strong> To measure system performance</li><li>Answer: <strong>B.</strong> Functional requirements should be measurable, because test cases should be developed directly from functional requirements. The same can be said about security and privacy requirements—all must be measurable because all should be tested.</li></ul></li></ol><h2 id="chapter-5-it-service-management-and-continuity" tabindex="-1"><a class="header-anchor" href="#chapter-5-it-service-management-and-continuity"><span>CHAPTER 5 IT Service Management and Continuity</span></a></h2><ol><li><p>A web application is displaying information incorrectly and many users have contacted the IT service desk. This matter should be considered a(n)</p><ul><li><strong>A.</strong> Incident</li><li><strong>B.</strong> Problem</li><li><strong>C.</strong> Bug</li><li><strong>D.</strong> Outage</li><li>Answer:<strong>B.</strong> A problem is defined as a condition that is the result of multiple incidents that exhibit common symptoms. In this example, many users are experiencing the effects of the application error.</li><li><strong>A. Incident</strong></li><li>Explanation: <ul><li>An incident is defined as an unplanned interruption to an IT service or a reduction in the quality of an IT service. In this case, the web application is displaying information incorrectly, causing disruptions for users. As a result, it qualifies as an incident that needs to be logged, diagnosed, and resolved by the IT service desk.</li></ul></li><li>Why the Other Options Are Incorrect: <ul><li>B. <strong>Problem</strong>: A problem is the underlying cause of one or more incidents. While this situation may later be investigated to determine the root cause (problem), the immediate issue is considered an incident.</li><li>C. <strong>Bug</strong>: A bug refers to a defect in the code or system. While the incorrect display might be caused by a bug, the term &quot;incident&quot; focuses on the service disruption from the users&#39; perspective.</li><li>D. <strong>Outage</strong>: An outage refers to a complete loss of service. In this case, the web application is still operational but displaying information incorrectly, so it does not qualify as an outage.</li></ul></li></ul></li><li><p>An IT organization is experiencing many cases of unexpected downtime that are caused by unauthorized changes to application code and operating system configuration. Which process should the IT organization implement to reduce downtime?</p><ul><li><strong>A.</strong> Configuration management</li><li><strong>B.</strong> Incident management</li><li><strong>C.</strong> Change management</li><li><strong>D.</strong> Problem management</li><li>Answer: <strong>C.</strong> Change management is the process of managing change through a life cycle process that consists of request, review, approve, implement, and verify.</li><li><strong>C. Change management</strong></li><li>Explanation: <ul><li>The change management process is designed to ensure that all changes to IT systems, such as application code or operating system configurations, are formally reviewed, approved, and documented before implementation. By implementing a structured change management process, the organization can:</li><li>Reduce unauthorized changes by requiring approval and tracking.</li><li>Minimize the risk of unexpected downtime caused by poorly planned or unapproved changes.</li><li>Improve accountability and transparency for all modifications to systems.</li></ul></li><li>Other Options: <ul><li>A. Configuration management: While configuration management tracks and maintains information about the system&#39;s setup, it does not directly address unauthorized changes.</li><li>B. Incident management: This focuses on resolving incidents (unexpected disruptions), not preventing downtime caused by unauthorized changes.</li><li>D. Problem management: This focuses on identifying and addressing the root cause of recurring incidents, but it does not control how changes are made.</li></ul></li></ul></li><li><p>An IT organization manages hundreds of servers, databases, and applications, and is having difficulty tracking changes to the configuration of these systems. What process should be implemented to remedy this?</p><ul><li><strong>A.</strong> Configuration management</li><li><strong>B.</strong> Change management</li><li><strong>C.</strong> Problem management</li><li><strong>D.</strong> Incident management</li><li>Answer: <strong>A.</strong> Configuration management is the process (often supplemented with automated tools) of tracking configuration changes to systems and system components such as databases and applications.</li><li><strong>A. Configuration management</strong></li><li>Explanation: <ul><li>Configuration management is the process of identifying, recording, and managing all components (hardware, software, and settings) in an IT environment. It involves maintaining an up-to-date Configuration Management Database (CMDB) or similar system to track changes, dependencies, and relationships among IT assets. This ensures: <ul><li>Accurate tracking of changes to configurations.</li><li>Improved visibility and control over the IT environment.</li><li>Easier troubleshooting and impact analysis.</li><li>For an organization with hundreds of servers, databases, and applications, configuration management is critical to managing complexity and ensuring consistency.</li></ul></li></ul></li><li>Other Options: <ul><li>B. Change management: While change management ensures changes are reviewed and approved, it does not track the configurations of systems. Configuration management complements change management by documenting those changes.</li><li>C. Problem management: This focuses on identifying and resolving the root causes of recurring incidents but does not track system configurations.</li><li>D. Incident management: This deals with resolving disruptions but does not manage system configurations.</li></ul></li></ul></li><li><p>A computer’s CPU, memory, and peripherals are connected to each other through a</p><ul><li><strong>A.</strong> Kernel</li><li><strong>B.</strong> FireWire</li><li><strong>C.</strong> Pipeline</li><li><strong>D.</strong> Bus</li><li>Answer: <strong>D.</strong> A bus connects all of the computer’s internal components together, including its CPU, main memory, secondary memory, and peripheral devices.</li><li><strong>D. Bus</strong></li><li>Explanation: <ul><li>A bus is the communication system that connects the various components of a computer, including the CPU, memory, and peripherals. It allows data to be transmitted between these components. There are different types of buses in a computer system, such as: <ul><li><strong>Data bus:</strong> Transfers data between components.</li><li><strong>Address bus:</strong> Specifies the memory locations for data transfers.</li><li><strong>Control bus:</strong> Manages control signals between the CPU and other components.</li></ul></li></ul></li><li>Other Options: <ul><li>A. <strong>Kernel</strong>: The kernel is the core of an operating system, managing hardware and software interactions, but it does not physically connect hardware components.</li><li>B. <strong>FireWire</strong>: This is a high-speed interface used to connect peripherals (like external storage) but is not a system-wide component interconnection.</li><li>C. <strong>Pipeline</strong>: Refers to CPU architecture and how instructions are processed, but it does not connect components physically.</li></ul></li></ul></li><li><p>A database administrator has been asked to configure a database management system so that it records all changes made by users. What should the database administrator implement?</p><ul><li><strong>A.</strong> Audit logging</li><li><strong>B.</strong> Triggers</li><li><strong>C.</strong> Stored procedures</li><li><strong>D.</strong> Journaling</li><li>Answer: <strong>A.</strong> The database administrator should implement audit logging. This will cause the database to record every change that is made to it.</li><li><strong>A. Audit logging</strong></li><li>Explanation: <ul><li>Audit logging is a feature in database management systems (DBMS) that records all changes made by users. It is typically used for tracking and monitoring user activities, ensuring accountability, and supporting compliance with security policies and regulations. Audit logs capture details such as: <ul><li>What changes were made.</li><li>Who made the changes.</li><li>When the changes were made.</li></ul></li><li>This helps in forensic analysis, monitoring unauthorized activities, and maintaining an accurate history of changes.</li></ul></li><li>Other Options: <ul><li>B. Triggers: Triggers are database mechanisms that automatically execute actions in response to specific events (e.g., insert, update, delete). While they can log changes, they are not a comprehensive audit logging solution.</li><li>C. Stored procedures: Stored procedures are precompiled SQL code stored in the database, used to perform repetitive tasks, but they do not inherently record user changes.</li><li>D. Journaling: Journaling typically refers to maintaining a log of transactions to ensure data integrity during recovery, but it is not specifically for tracking user changes.</li></ul></li></ul></li><li><p>The layers of the TCP/IP reference model are</p><ul><li><strong>A.</strong> Link, Internet, transport, application</li><li><strong>B.</strong> Physical, link, Internet, transport, application</li><li><strong>C.</strong> Link, transport, Internet, application</li><li><strong>D.</strong> Physical, data link, network, transport, session, presentation, application</li><li>Answer: <strong>A.</strong> The layers of the TCP/IP model are (from lowest to highest) link, Internet, transport, and application.</li><li><strong>A. Link, Internet, Transport, Application</strong></li><li>Explanation: <ul><li>The TCP/IP reference model has four layers, each corresponding to specific functionality in the communication process: <ul><li><strong>Link Layer</strong>: Responsible for handling the physical transmission of data across network devices. It corresponds to the physical and data link layers in the OSI model.</li><li><strong>Internet Layer</strong>: Handles addressing, routing, and delivering packets between devices across different networks (e.g., IP protocol).</li><li><strong>Transport Layer</strong>: Ensures reliable data transmission between devices, providing error checking and flow control (e.g., TCP, UDP).</li><li><strong>Application Layer</strong>: Provides network services directly to applications, enabling end-user communication (e.g., HTTP, SMTP).</li></ul></li></ul></li><li>Other Options: <ul><li>B. Physical, link, Internet, transport, application: The TCP/IP model combines physical and data link layers into the Link layer, so this option is incorrect.</li><li>C. Link, transport, Internet, application: Incorrect layer sequence; the Internet layer should come before the Transport layer.</li><li>D. Physical, data link, network, transport, session, presentation, application: This describes the 7-layer OSI model, not the 4-layer TCP/IP model.</li></ul></li></ul></li><li><p>The purpose of the Internet layer in the TCP/IP model is</p><ul><li><strong>A.</strong> Encapsulation</li><li><strong>B.</strong> Packet delivery on a local network</li><li><strong>C.</strong> Packet delivery on a local or remote network</li><li><strong>D.</strong> Order of delivery and flow control</li><li>Answer: <strong>C.</strong> The purpose of the Internet layer in the TCP/IP model is the delivery of packets from one station to another, on the same network or on a different network.</li><li><strong>C. Packet delivery on a local or remote network</strong></li><li>Explanation: <ul><li>The Internet layer in the TCP/IP model is responsible for: <ul><li><strong>Routing and forwarding packets</strong>: Ensuring that data is delivered between the source and destination across different networks.</li><li><strong>Addressing</strong>: Using IP addresses to identify devices on a network.</li><li><strong>Fragmentation and reassembly</strong>: Dividing packets if necessary and ensuring they are reassembled at the destination.</li><li>The Internet layer works across <strong>both local and remote networks</strong> by providing routing capabilities, enabling data to traverse multiple networks to reach its destination.</li></ul></li></ul></li><li>Other Options: <ul><li>A. Encapsulation: Encapsulation is a general process performed at multiple layers, not specific to the Internet layer.</li><li>B. Packet delivery on a local network: This is primarily handled by the Link layer.</li><li>D. Order of delivery and flow control: These are responsibilities of the Transport layer (e.g., TCP).</li></ul></li></ul></li><li><p>The purpose of the DHCP protocol is</p><ul><li><strong>A.</strong> Control flow on a congested network.</li><li><strong>B.</strong> Query a station to discover its IP address.</li><li><strong>C.</strong> Assign an IP address to a station.</li><li><strong>D.</strong> Assign an Ethernet MAC address to a station.</li><li>Answer: <strong>C.</strong> The DHCP protocol is used to assign IP addresses to computers on a network.</li><li>C. Assign an IP address to a station.</li><li>Explanation: <ul><li>The Dynamic Host Configuration Protocol (DHCP) is used to dynamically assign IP addresses to devices (stations) on a network. DHCP automates the configuration of network settings, such as: <ul><li>IP address: Assigning a unique IP address to each device.</li><li>Subnet mask: Defining the network and host portions of the IP address.</li><li>Default gateway: Providing the router address for devices to access other networks.</li><li>DNS servers: Assigning the addresses of servers that resolve domain names.</li><li>This reduces manual configuration and ensures efficient use of available IP addresses.</li></ul></li></ul></li><li>Other Options: <ul><li>A. Control flow on a congested network: This is not the purpose of DHCP; flow control is handled by protocols like TCP or network QoS mechanisms.</li><li>B. Query a station to discover its IP address: This is a function of protocols like ARP (Address Resolution Protocol) or ICMP.</li><li>D. Assign an Ethernet MAC address to a station: MAC addresses are hardware-specific and do not require assignment by DHCP.</li></ul></li></ul></li><li><p>An IS auditor is examining a wireless (Wi-Fi) network and has determined that the network uses WEP encryption. What action should the auditor take?</p><ul><li><strong>A.</strong> Recommend that encryption be changed to WPA.</li><li><strong>B.</strong> Recommend that encryption be changed to EAP.</li><li><strong>C.</strong> Request documentation for the key management process.</li><li><strong>D.</strong> Request documentation for the authentication process.</li><li>Answer: <strong>A.</strong> The WEP protocol has been seriously compromised and should be replaced with WPA or WPA2 encryption.</li><li><strong>A. Recommend that encryption be changed to WPA.</strong></li><li>Explanation: <ul><li>Wired Equivalent Privacy (WEP) is an outdated and insecure encryption protocol for Wi-Fi networks. It is vulnerable to attacks, and its use poses significant security risks. Modern encryption standards, such as Wi-Fi Protected Access (WPA) or preferably WPA2/WPA3, provide stronger encryption and are recommended to protect wireless networks.</li><li>The IS auditor should recommend upgrading the encryption to a more secure standard to enhance the overall security of the wireless network.</li></ul></li><li>Why the Other Options Are Incorrect: <ul><li>B. Recommend that encryption be changed to EAP: Extensible Authentication Protocol (EAP) is an authentication framework, not an encryption standard.</li><li>C. Request documentation for the key management process: Key management is important but does not address the fundamental issue of WEP being insecure.</li><li>D. Request documentation for the authentication process: While useful, it does not solve the encryption vulnerability posed by WEP.</li></ul></li></ul></li><li><p>126.0.0.1 is an example of a</p><ul><li><strong>A.</strong> MAC address</li><li><strong>B.</strong> Loopback address</li><li><strong>C.</strong> Class A address</li><li><strong>D.</strong> Subnet mask</li><li>Answer: <strong>C.</strong> Class A addresses are in the range 0.0.0.0 to 127.255.255.255. The address 126.0.0.1 falls into this range.</li><li><strong>C. Class A address</strong></li><li>Explanation: <ul><li>126.0.0.1 is part of the Class A address range, which spans from 1.0.0.0 to 126.255.255.255. This address can be assigned to network devices as part of a public or private network.</li></ul></li><li>Why It&#39;s Not a Loopback Address: <ul><li>The loopback address range is strictly 127.0.0.0 to 127.255.255.255.</li><li>Loopback addresses are reserved for testing and local communication within the same device. They are not routable or usable on external networks.</li><li>Since 126.0.0.1 falls outside the 127.x.x.x range, it is not a loopback address.</li></ul></li><li>Other Options: <ul><li>A. <strong>MAC address</strong>: This refers to a hardware identifier and does not resemble an IPv4 address like 126.0.0.1.</li><li>B. <strong>Loopback address</strong>: Incorrect, as explained above.</li><li>D. <strong>Subnet mask</strong>: Subnet masks (e.g., 255.255.255.0) are not valid IP addresses but rather define the network and host portions of an IP address.</li></ul></li></ul></li><li><p>What is the most important consideration when selecting a hot site?</p><ul><li><strong>A.</strong> Time zone</li><li><strong>B.</strong> Geographic location in relation to the primary site</li><li><strong>C.</strong> Proximity to major transportation</li><li><strong>D.</strong> Natural hazards</li><li>Answer: <strong>B</strong>. An important selection criterion for a hot site is the geographic location in relation to the primary site. If they are too close together, then a single disaster event may involve both locations.</li><li>B. Geographic location in relation to the primary site</li><li>Explanation: <ul><li>When selecting a hot site (a fully operational backup site ready to take over in case of a disaster), the geographic location in relation to the primary site is the most critical factor. This ensures:</li><li>Risk Mitigation: The hot site should be far enough away to avoid being affected by the same disaster (e.g., earthquakes, hurricanes, power outages).</li><li>Accessibility: It should still be accessible to personnel and resources when needed.</li><li>The balance between proximity (for ease of access) and distance (to reduce shared risks) is key.</li></ul></li><li>Other Options: <ul><li>A. Time zone: While important for global operations, it is secondary to ensuring physical risk separation and accessibility.</li><li>C. Proximity to major transportation: This can improve logistics but is not as critical as the site&#39;s ability to function during a disaster.</li><li>D. Natural hazards: Avoiding natural hazards is important, but it is part of the overall geographic location consideration.</li></ul></li></ul></li><li><p>An organization has established a recovery point objective of 14 days for its most critical business applications. Which recovery strategy would be the best choice?</p><ul><li><strong>A.</strong> Mobile site</li><li><strong>B.</strong> Warm site</li><li><strong>C.</strong> Hot site</li><li><strong>D.</strong> Cold site</li><li>Answer: <strong>D.</strong> An organization that has a 14-day recovery time objective (RTO) can use a cold site for its recovery strategy. Fourteen days is enough time for most organizations to acquire hardware and recover applications.</li><li><strong>D. Cold site</strong></li><li>Explanation: <ul><li>A Recovery Point Objective (RPO) of 14 days means the organization can tolerate up to 14 days of data loss. This indicates a relatively low urgency for immediate restoration of systems. A cold site is the most cost-effective option for such scenarios because:</li></ul></li><li>Cold site characteristics: <ul><li>Typically includes basic infrastructure (e.g., power, cooling, physical space).</li><li>Does not include pre-installed hardware or software.</li><li>Requires significant time to become operational, aligning well with a 14-day RPO.</li><li>For an RPO of 14 days, a cold site provides adequate recovery capabilities without the higher costs associated with warm or hot sites, which are designed for shorter RPOs and faster recovery times.</li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. Mobile site: A mobile site (a portable data center) is suitable for short RPOs but is generally more expensive and not necessary for a 14-day RPO.</li><li>B. Warm site: A warm site is partially equipped and can be operational faster than a cold site. It is more suited for shorter RPOs.</li><li>C. Hot site: A hot site is fully operational and ready for immediate use, ideal for near-zero RPOs. It would be overkill and too expensive for a 14-day RPO.</li></ul></li></ul></li><li><p>What technology should an organization use for its application servers to provide continuous service to users?</p><ul><li><strong>A.</strong> Dual power supplies</li><li><strong>B.</strong> Server clustering</li><li><strong>C.</strong> Dual network feeds</li><li><strong>D.</strong> Transaction monitoring</li><li>Answer: <strong>B.</strong> An organization that wants its application servers to be continuously available to its users needs to employ server clustering. This enables at least one server to be always available to service user requests.</li><li>B. Server clustering</li><li>Explanation: <ul><li>Server clustering is a technology that connects multiple servers to work together as a single system to provide high availability, load balancing, and fault tolerance for application servers. If one server in the cluster fails, another server takes over the workload, ensuring continuous service to users without significant downtime.</li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. Dual power supplies: Dual power supplies provide redundancy for power failures but do not ensure continuous application service if the server itself fails.</li><li>C. Dual network feeds: Redundant network feeds prevent network outages but do not protect against server failures.</li><li>D. Transaction monitoring: Transaction monitoring tracks and logs transactions for analysis but does not ensure service continuity.</li></ul></li></ul></li><li><p>An organization currently stores its backup media in a cabinet next to the computers being backed up. A consultant told the organization to store backup media at an off-site storage facility. What risk did the consultant most likely have in mind when he made this recommendation?</p><ul><li><strong>A.</strong> A disaster that damages computer systems can also damage backup media.</li><li><strong>B.</strong> Backup media rotation may result in loss of data backed up several weeks in the past.</li><li><strong>C.</strong> Corruption of online data will require rapid data recovery from off-site storage.</li><li><strong>D.</strong> Physical controls at the data processing site are insufficient.</li><li>Answer: <strong>A.</strong> The primary reason for employing off-site backup media storage is to mitigate the effects of a disaster that could otherwise destroy computer systems and their backup media.</li><li><strong>A. A disaster that damages computer systems can also damage backup media.</strong></li><li>Explanation: <ul><li>Storing backup media in the same physical location as the computers being backed up creates a single point of failure. If a disaster such as a fire, flood, or earthquake occurs, it can destroy both the computer systems and the backup media, rendering data recovery impossible. By storing backup media at an off-site storage facility, the organization ensures that backups are protected from localized disasters.</li></ul></li><li>Why Other Options Are Incorrect: <ul><li>B. Backup media rotation may result in loss of data backed up several weeks in the past: This is related to retention policies and is not the primary risk addressed by off-site storage.</li><li>C. Corruption of online data will require rapid data recovery from off-site storage: While off-site storage supports recovery, it is typically not the fastest solution for immediate restoration and is not the primary reason for the recommendation.</li><li>D. Physical controls at the data processing site are insufficient: Although physical controls are important, the main risk the consultant is addressing is disaster recovery, not physical security.</li></ul></li></ul></li><li><p>Which of the following statements about virtual server hardening is true?</p><ul><li><strong>A.</strong> The configuration of the host operating system will automatically flow to each guest operating system.</li><li><strong>B.</strong> Each guest virtual machine needs to be hardened separately.</li><li><strong>C.</strong> Guest operating systems do not need to be hardened because they are protected by the hypervisor.</li><li><strong>D.</strong> Virtual servers do not need to be hardened because they do not run directly on computer hardware.</li><li>Answer: <strong>B.</strong> In a virtualization environment, each guest operating system needs to be hardened; they are no different from operating systems running directly on server (or workstation) hardware.</li><li><strong>B. Each guest virtual machine needs to be hardened separately.</strong></li><li>Explanation: <ul><li>In a virtualized environment, each guest virtual machine (VM) operates as an independent system, even though they share the same physical hardware and hypervisor. Hardening each guest VM is essential to: <ul><li>Reduce vulnerabilities and risks within the guest operating system (OS).</li><li>Protect against attacks or breaches that could spread to other VMs or the host.</li><li>Hardening typically involves disabling unnecessary services, applying patches, securing network configurations, and implementing strict access controls.</li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. The configuration of the host operating system will automatically flow to each guest operating system: This is incorrect. Guest VMs operate independently, and their configurations must be managed separately.</li><li>C. Guest operating systems do not need to be hardened because they are protected by the hypervisor: While the hypervisor provides some isolation, it cannot fully protect a vulnerable guest OS from being exploited.</li><li>D. Virtual servers do not need to be hardened because they do not run directly on computer hardware: This is incorrect; the virtual environment still requires security measures for each VM.</li></ul></li></ul></li></ol><h2 id="chapter-6-information-asset-protection" tabindex="-1"><a class="header-anchor" href="#chapter-6-information-asset-protection"><span>CHAPTER 6 Information Asset Protection</span></a></h2><ol><li>A fire sprinkler system has water in its pipes, and sprinkler heads emit water only if the ambient temperature reaches 220°F. What type of system is this? <ul><li><strong>A.</strong> Deluge</li><li><strong>B.</strong> Post-action</li><li><strong>C.</strong> Wet pipe</li><li><strong>D.</strong> Pre-action</li><li>Answer: <strong>C.</strong> A wet pipe fire sprinkler system is charged with water and will discharge water out of any sprinkler head whose fuse has reached a preset temperature.</li><li><strong>C. Wet pipe</strong></li><li>Explanation: <ul><li>A wet pipe sprinkler system is the most common type of fire sprinkler system. It has water continuously stored in its pipes, ready to be released immediately when a sprinkler head is activated due to heat. In this case, the sprinkler heads emit water when the ambient temperature reaches 220°F, a characteristic of wet pipe systems.</li></ul></li><li>Other Types of Systems:</li><li>A. Deluge: <ul><li>Pipes are empty until activated, and all sprinkler heads discharge water simultaneously when a fire is detected.</li><li>Typically used in high-hazard areas, such as chemical storage or aircraft hangars.</li></ul></li><li>B. Post-action: <ul><li>This is not a recognized fire sprinkler system type.</li></ul></li><li>D. Pre-action: <ul><li>Pipes are empty until an initial fire detection system (e.g., smoke or heat sensors) activates, filling the pipes with water.</li><li>Water is released only if a second event occurs (e.g., sprinkler head activation).</li><li>Often used in areas where accidental discharge could cause significant damage, like data centers or museums.</li></ul></li></ul></li><li>An organization is building a data center in an area frequented by power outages. The organization cannot tolerate power outages. What power system controls should be selected? <ul><li><strong>A.</strong> Uninterruptible power supply and electric generator</li><li><strong>B.</strong> Uninterruptible power supply and batteries</li><li><strong>C.</strong> Electric generator</li><li><strong>D.</strong> Electric generator and line conditioning</li><li>Answer: <strong>A.</strong> The best solution is an electric generator and an uninterruptible power supply (UPS). A UPS responds to a power outage by providing continuous electric power without interruption. An electric generator provides backup power for extended periods.</li><li><strong>A. Uninterruptible power supply and electric generator</strong></li><li>Explanation: <ul><li>To ensure continuous power in a data center located in an area prone to power outages, a combination of an Uninterruptible Power Supply (UPS) and an electric generator is the best choice: <ul><li>Uninterruptible Power Supply (UPS): <ul><li>Provides immediate, short-term power during an outage, preventing disruptions until a backup generator starts.</li><li>Protects sensitive equipment from power fluctuations.</li></ul></li><li>Electric Generator: <ul><li>Provides long-term backup power during extended outages.</li><li>Automatically activates once the UPS takes over, ensuring seamless continuity.</li><li>This combination addresses both short-term and long-term power outages, ensuring the organization can maintain critical operations.</li></ul></li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>B. Uninterruptible power supply and batteries: While a UPS includes batteries, they alone are insufficient for extended outages, as they only provide power for a short duration.</li><li>C. Electric generator: A generator takes time to start during an outage, leaving a gap in power supply. A UPS is needed to bridge this gap.</li><li>D. Electric generator and line conditioning: Line conditioning protects against voltage spikes and dips but does not provide backup power during outages.</li></ul></li></ul></li><li>An auditor has discovered several errors in user account management: many terminated employees’ computer accounts are still active. What is the best course of action? <ul><li><strong>A.</strong> Improve the employee termination process.</li><li><strong>B.</strong> Shift responsibility for employee terminations to another group.</li><li><strong>C.</strong> Audit the process more frequently.</li><li><strong>D.</strong> Improve the employee termination process and audit the process more frequently.</li><li>Answer: <strong>D.</strong> The best course of action is to improve the employee termination process to reduce the number of exceptions. For a time, the process should be audited more frequently to make sure that the improvement is effective.</li><li><strong>D. Improve the employee termination process and audit the process more frequently.</strong></li><li>Explanation: <ul><li>Improve the employee termination process: <ul><li>The primary issue is that terminated employees&#39; accounts are not being disabled or deleted promptly, which poses a significant security risk.</li><li>Enhancing the termination process ensures that user account deactivation is integrated into the workflow, reducing the risk of unauthorized access.</li></ul></li><li>Audit the process more frequently: <ul><li>Regular audits ensure that the improved process is being followed consistently.</li><li>This helps identify any lapses or deviations from the policy in a timely manner.</li></ul></li><li>Both actions are necessary to address the root cause (a flawed termination process) and to provide ongoing assurance through monitoring and audits.</li></ul></li><li>Why Other Options Are Insufficient: <ul><li>A. Improve the employee termination process: While improving the process is essential, it alone may not catch future lapses without regular audits.</li><li>B. Shift responsibility for employee terminations to another group: Changing responsibility does not address the root cause or ensure proper execution.</li><li>C. Audit the process more frequently: Auditing alone will not fix the underlying flaws in the termination process.</li></ul></li></ul></li><li>An auditor has discovered that several administrators in an application share an administrative account. What course of action should the auditor recommend? <ul><li><strong>A.</strong> Implement activity logging on the administrative account.</li><li><strong>B.</strong> Use several named administrative accounts that are not shared.</li><li><strong>C.</strong> Implement a host-based intrusion detection system.</li><li><strong>D.</strong> Require each administrator to sign nondisclosure and acceptable-use agreements.</li><li>Answer: <strong>B.</strong> Several separate administrative accounts should be used. This will enforce accountability for each administrator’s actions.</li><li><strong>B. Use several named administrative accounts that are not shared.</strong></li><li>Explanation: <ul><li>Sharing administrative accounts undermines accountability because it becomes impossible to determine who performed specific actions. To address this, each administrator should have a unique named administrative account. This ensures: <ul><li><strong>Accountability</strong>: Actions can be directly traced back to the individual responsible.</li><li><strong>Auditing</strong>: Logs can accurately reflect who made specific changes.</li><li><strong>Compliance</strong>: Many regulations and security frameworks (e.g., ISO 27001, NIST) require unique user accounts for privileged access.</li></ul></li></ul></li><li>Why Other Options Are Insufficient: <ul><li>A. Implement activity logging on the administrative account: Logging is important but does not solve the accountability problem if multiple people are using the same account.</li><li>C. Implement a host-based intrusion detection system: While intrusion detection can help identify malicious activity, it does not address the root issue of shared accounts.</li><li>D. Require each administrator to sign nondisclosure and acceptable-use agreements: Agreements are important for legal and policy purposes but do not provide technical accountability.</li></ul></li></ul></li><li>An organization that has experienced a sudden increase in its long-distance charges has asked an auditor to investigate. What activity is the auditor likely to suspect is responsible for this? <ul><li><strong>A.</strong> Employees making more long-distance calls</li><li><strong>B.</strong> Toll fraud</li><li><strong>C.</strong> PBX malfunction</li><li><strong>D.</strong> Malware in the PBX</li><li>Answer: <strong>B.</strong> The auditor is most likely to suspect that intruders have discovered a vulnerability in the organization’s PBX and are committing toll fraud.</li><li><strong>B. Toll fraud</strong></li><li>Explanation: <ul><li>Toll fraud is a common cause of unexpected increases in long-distance charges. It occurs when unauthorized individuals gain access to a <strong>private branch exchange (PBX)</strong> or telecommunication system and make unauthorized long-distance or international calls, often at the expense of the organization. Toll fraud is a well-known security risk associated with poorly configured or unsecured telephony systems.</li></ul></li><li>Why Other Options Are Less Likely: <ul><li>A. Employees making more long-distance calls: While this could contribute, a sudden and significant increase in charges is more likely due to malicious activity rather than legitimate employee usage.</li><li>C. PBX malfunction: A malfunction might cause operational issues, but it is unlikely to directly lead to a surge in long-distance charges.</li><li>D. Malware in the PBX: Although possible, malware affecting PBX systems is less common than toll fraud as the direct cause of increased charges.</li></ul></li></ul></li><li>An auditor is examining a key management process and has found that the IT department is not following its split-custody procedure. What is the likely result of this failure? <ul><li><strong>A.</strong> One or more individuals are in possession of the entire password for an encryption key.</li><li><strong>B.</strong> One or more individuals are in possession of encrypted files.</li><li><strong>C.</strong> Backup tapes are not being stored at an off-site facility.</li><li><strong>D.</strong> Two or more employees are sharing an administrative account.</li><li>Answer: <strong>A.</strong> Someone may be in possession of the entire password for an encryption key. For instance, split custody requires that a password be broken into two or more parts, where each part is in possession of a unique individual. This prevents any one individual from having an entire password.</li><li><strong>A. One or more individuals are in possession of the entire password for an encryption key.</strong></li><li>Explanation: <ul><li>Split custody is a key management practice where no single individual has full control over an entire encryption key or its associated password. The key or password is divided among multiple parties to ensure that no one person can misuse the encryption key independently. If the split-custody procedure is not followed, it means: <ul><li>One or more individuals might have access to the entire password for an encryption key.</li><li>This introduces a significant security risk, as it violates the principle of separation of duties, potentially leading to unauthorized decryption or misuse of sensitive data.</li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>B. One or more individuals are in possession of encrypted files: This is not related to split custody, as encrypted files can often be shared securely without splitting keys.</li><li>C. Backup tapes are not being stored at an off-site facility: This relates to backup and disaster recovery practices, not key management or split custody.</li><li>D. Two or more employees are sharing an administrative account: This is an account management issue, not related to the failure of split-custody in key management.</li></ul></li></ul></li><li>A developer is updating an application that saves passwords in plaintext. What is the best method for securely storing passwords? <ul><li><strong>A.</strong> Encrypted with each user’s public key</li><li><strong>B.</strong> Encrypted with a public key</li><li><strong>C.</strong> Encrypted with a private key</li><li><strong>D.</strong> Hashed</li><li>Answer: <strong>D.</strong> Passwords should be stored as a hash. This makes it nearly impossible for any person to retrieve a password, which could lead to account compromise.</li><li><strong>D. Hashed</strong></li><li>Explanation: <ul><li>The best practice for securely storing passwords is to use a hashing algorithm. Hashing converts passwords into fixed-length, irreversible representations (hash values). When a user attempts to log in, the entered password is hashed and compared to the stored hash.</li><li>Key Benefits of Hashing: <ul><li><strong>Irreversibility</strong>: Unlike encryption, hashing is a one-way process. The original password cannot be derived from the hash.</li><li><strong>Security with Salting</strong>: Adding a unique &quot;salt&quot; to each password before hashing prevents attacks like precomputed dictionary attacks or rainbow table attacks.</li><li><strong>Scalability</strong>: Modern hashing algorithms (e.g., bcrypt, Argon2, or PBKDF2) are designed to be computationally expensive, making brute-force attacks more difficult.</li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. Encrypted with each user’s public key: <ul><li>Public key encryption is reversible with the private key and not suited for password storage.</li><li>It adds unnecessary complexity.</li></ul></li><li>B. Encrypted with a public key: <ul><li>Similar to A, encryption is not appropriate because encryption is reversible.</li></ul></li><li>C. Encrypted with a private key: <ul><li>Storing passwords encrypted with a private key is risky because if the key is compromised, all passwords are exposed.</li><li>Passwords should not rely on reversible encryption for storage.</li></ul></li></ul></li></ul></li><li>An organization experiences frequent malware infections on end-user workstations that are received through e-mail, despite the fact that workstations have anti-malware software. What is the best measure for reducing malware? <ul><li><strong>A.</strong> Anti-malware software on web proxy servers</li><li><strong>B.</strong> Firewalls</li><li><strong>C.</strong> Anti-malware software on e-mail servers</li><li><strong>D.</strong> Intrusion prevention systems</li><li>Answer: <strong>C.</strong> Implementing anti-malware software on e-mail servers will provide an effective defense-in-depth, which should help to reduce the number of malware attacks on end-user workstations.</li><li><strong>C. Anti-malware software on e-mail servers</strong></li><li>Explanation: <ul><li>The best measure to reduce malware infections from email is to deploy anti-malware software on e-mail servers. This approach ensures that emails are scanned for malware before they reach end-user workstations. By stopping malware at the source, the organization can: <ul><li>Prevent malicious attachments or links from reaching users.</li><li>Reduce the reliance on end-users and endpoint protections as the first line of defense.</li></ul></li></ul></li><li>Why Other Options Are Less Effective: <ul><li>A. Anti-malware software on web proxy servers: This is useful for detecting malware from web traffic but does not address email-based malware.</li><li>B. Firewalls: Firewalls filter network traffic based on rules but are not designed to detect or remove malware in email messages.</li><li>D. Intrusion prevention systems: IPS can detect and block malicious network traffic but is not focused on scanning email attachments or links for malware.</li></ul></li></ul></li><li>An auditor has reviewed the access privileges of some employees and has discovered that employees with longer terms of service have excessive privileges. What can the auditor conclude from this? <ul><li><strong>A.</strong> Employee privileges are not being removed when they transfer from one position to another.</li><li><strong>B.</strong> Long-time employees are able to guess other users’ passwords successfully and add to their privileges.</li><li><strong>C.</strong> Long-time employees’ passwords should be set to expire more frequently.</li><li><strong>D.</strong> The organization’s termination process is ineffective.</li><li>Answer: <strong>A.</strong> User privileges are not being removed from their old position when they transfer to a new position. This results in employees with excessive privileges.</li><li><strong>A. Employee privileges are not being removed when they transfer from one position to another.</strong></li><li>Explanation: <ul><li>When employees accumulate excessive privileges over time, it often indicates that privilege management is not being handled properly, particularly during role transitions. As employees move to new positions, old privileges are not revoked, leading to an accumulation of unnecessary access rights, a condition known as <strong>privilege creep</strong>.</li></ul></li><li>Key Points: <ul><li>This situation often arises due to inadequate monitoring or failure to follow access control policies during employee role changes.</li><li>Excessive privileges pose a security risk, as they can enable unauthorized access to sensitive data or systems.</li><li>Why Other Options Are Incorrect: <ul><li>B. Long-time employees are able to guess other users’ passwords successfully and add to their privileges: <ul><li>Privileges are generally controlled by access management systems, not by users being able to guess passwords and modify their own privileges.</li></ul></li><li>C. Long-time employees’ passwords should be set to expire more frequently: <ul><li>Password expiration is unrelated to the accumulation of privileges.</li></ul></li><li>D. The organization’s termination process is ineffective: <ul><li>The issue here is not about terminated employees but about managing active employees’ access as they change roles.</li></ul></li></ul></li></ul></li></ul></li><li>An organization wants to reduce the number of user IDs and passwords that its employees need to remember. What is the best available solution to this problem? <ul><li><strong>A.</strong> Password vaults for storing user IDs and passwords</li><li><strong>B.</strong> Token authentication</li><li><strong>C.</strong> Single sign-on</li><li><strong>D.</strong> Reduced sign-on</li><li>Answer: <strong>D.</strong> The most direct solution to the problem of too many user credentials is reduced sign-on. This provides a single authentication service (such as LDAP or Active Directory) that many applications can use for centralized user authentication.</li><li><strong>C. Single sign-on</strong></li><li>Explanation: <ul><li>Single sign-on (SSO) is a solution that allows employees to access multiple applications and systems using a single set of credentials (user ID and password). With SSO: <ul><li>Employees only need to remember one password, reducing cognitive load and improving usability.</li><li>Authentication is centralized, making access management and auditing easier for administrators.</li><li>Security improves by reducing the likelihood of weak or reused passwords across multiple systems.</li></ul></li><li>Key Benefits of SSO: <ul><li>Enhances user experience by simplifying login processes.</li><li>Reduces password-related IT support issues, such as password resets.</li><li>Improves security when implemented with multi-factor authentication (MFA).</li></ul></li></ul></li><li>Why Other Options Are Less Effective: <ul><li>A. Password vaults for storing user IDs and passwords: <ul><li>While password managers help store and autofill passwords securely, they don&#39;t reduce the number of credentials users need to manage.</li></ul></li><li>B. Token authentication: <ul><li>Token-based authentication enhances security but does not address the need to reduce the number of user IDs and passwords.</li></ul></li><li>D. Reduced sign-on: <ul><li>Reduced sign-on refers to limiting the number of systems requiring separate authentication, but it is less comprehensive than SSO, which provides seamless access across multiple systems.</li></ul></li></ul></li></ul></li><li>An IS auditor has discovered that an employee has installed a Wi-Fi access point in his cube. What action should the IS auditor take? <ul><li><strong>A.</strong> The IS auditor should include this in his audit report.</li><li><strong>B.</strong> The IS auditor should immediately report this as a high-risk situation.</li><li><strong>C.</strong> The IS auditor should ask the employee to turn off the Wi-Fi access point when it is not being used.</li><li><strong>D.</strong> The IS auditor should test the Wi-Fi access point to see whether it properly authenticates users.</li><li>Answer: <strong>B.</strong> Finding an unauthorized access point is a high-risk situation that the IS auditor should report immediately to management.</li><li><strong>B. The IS auditor should immediately report this as a high-risk situation.</strong></li><li>Explanation: <ul><li>An unauthorized Wi-Fi access point installed by an employee is a serious security risk. It creates a potential backdoor for attackers to access the organization&#39;s network, bypassing security controls such as firewalls or intrusion detection systems. This is often referred to as a rogue access point, and it can: <ul><li>Compromise network security by providing unauthorized access.</li><li>Bypass corporate security policies.</li><li>Expose sensitive data to unauthorized users.</li><li>The IS auditor must escalate this as a high-risk situation to ensure immediate corrective actions, such as disabling the device, investigating the employee&#39;s intent, and reinforcing security policies.</li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. The IS auditor should include this in his audit report: <ul><li>While the issue should be documented in the report, immediate action is required to mitigate the risk before the audit concludes.</li></ul></li><li>C. The IS auditor should ask the employee to turn off the Wi-Fi access point when it is not being used: <ul><li>This is insufficient, as the device still poses a security risk even if turned off temporarily. Unauthorized devices must be removed entirely.</li></ul></li><li>D. The IS auditor should test the Wi-Fi access point to see whether it properly authenticates users: <ul><li>Testing the device does not address the underlying issue of unauthorized installation. Unauthorized access points must be removed immediately.</li></ul></li></ul></li></ul></li><li>An auditor is examining an organization’s data loss prevention (DLP) system. The DLP system is recording instances of sensitive information that is leaving the organization. There are no records of actions taken. What should the IS auditor recommend? <ul><li><strong>A.</strong> That management appoint a party responsible for taking action when the DLP system detects that sensitive information is leaving the organization</li><li><strong>B.</strong> That management develop procedures for responding to DLP system alerts</li><li><strong>C.</strong> That management discontinue use of the DLP system since no one is taking action</li><li><strong>D.</strong> That the DLP system be reconfigured to stop issuing alerts</li><li>Answer: <strong>A.</strong> An organization using a DLP system should be acting on alerts that the DLP system generates in order to curb employee and system behavior.</li><li>B. That management develop procedures for responding to DLP system alerts</li><li>Explanation: <ul><li>A Data Loss Prevention (DLP) system is effective only if there are clear procedures for responding to its alerts. The system’s purpose is to detect and report sensitive data leakage, but the absence of action in response to alerts diminishes its value. The auditor should recommend that management establish formalized procedures to:</li><li>Define roles and responsibilities for investigating and addressing alerts.</li><li>Specify actions to be taken when sensitive data leakage is detected.</li><li>Ensure proper escalation and resolution of incidents.</li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. That management appoint a party responsible for taking action when the DLP system detects that sensitive information is leaving the organization: <ul><li>Assigning responsibility is important, but it must be accompanied by structured procedures to guide the response process effectively.</li></ul></li><li>C. That management discontinue use of the DLP system since no one is taking action: <ul><li>Disabling the DLP system removes a critical layer of data protection and exposes the organization to greater risks.</li></ul></li><li>D. That the DLP system be reconfigured to stop issuing alerts: <ul><li>Ignoring alerts defeats the purpose of having a DLP system and leaves sensitive data vulnerabilities unaddressed.</li></ul></li></ul></li><li>Recommended Actions: <ul><li>Develop procedures: Create detailed guidelines for responding to DLP alerts.</li><li>Train staff: Ensure that employees know how to handle DLP alerts.</li><li>Assign responsibility: Designate individuals or teams to monitor and act on DLP alerts.</li><li>Monitor compliance: Conduct regular reviews to ensure procedures are followed.</li></ul></li><li>Why B Is Better Than A: <ul><li>A Procedural Gap: <ul><li>The question explicitly mentions that there are no records of actions taken. This indicates a procedural issue, not just a lack of assigned responsibility.</li><li>Assigning responsibility alone (as A suggests) is insufficient unless accompanied by clear, formalized procedures outlining what actions should be taken and how.</li></ul></li><li>Comprehensive Solution: <ul><li>B emphasizes the creation of procedures for responding to DLP alerts. This is the root solution to ensure both: <ul><li>Actions are taken when alerts occur.</li><li>Responsibilities are clearly defined within a structured framework.</li></ul></li></ul></li><li>Best Practice Alignment: <ul><li>Security frameworks (e.g., NIST, ISO 27001) prioritize procedural clarity for effective incident response. Procedures ensure consistent, scalable, and measurable responses to security events.</li></ul></li></ul></li></ul></li><li>An organization’s remote access requires a user ID and one-time password token. What weakness does this scheme have? <ul><li><strong>A.</strong> Someone who finds a one-time password token could log in as the user by guessing the password.</li><li><strong>B.</strong> Someone who finds a one-time password token could log in as the user by guessing the user ID.</li><li><strong>C.</strong> Someone who knows the user ID could derive the password.</li><li><strong>D.</strong> Someone who is able to eavesdrop on the authentication can log in later using a replay attack.</li><li>Answer: <strong>B.</strong> Someone who finds a one-time password token and then tries to log in to a system and discovers that the site does not request a password could guess the user ID and possibly be able to log in to the system.</li><li><strong>D. Someone who is able to eavesdrop on the authentication can log in later using a replay attack.</strong></li><li>Explanation: <ul><li>A user ID and one-time password (OTP) token scheme provides an extra layer of security for remote access. However, it is vulnerable to replay attacks if the authentication process does not include mechanisms to prevent reuse of the captured credentials.</li><li>Review: <ul><li>Highly plausible and correct. OTP tokens are vulnerable to replay attacks if the system doesn’t implement protections like unique session IDs or one-time session validity.</li><li>Attackers could intercept a valid OTP during transmission and use it later if replay prevention mechanisms (e.g., timestamps or session expiration) are not in place.</li></ul></li></ul></li><li>Replay Attack: <ul><li>An attacker intercepts the user ID and the one-time password during authentication.</li><li>If the system does not implement protections like session expiration or nonces (unique values for each session), the attacker can reuse the intercepted credentials to gain unauthorized access.</li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. Someone who finds a one-time password token could log in as the user by guessing the password: <ul><li>OTP tokens generate unique passwords that are valid for only a short time. Guessing the static user ID or password is not feasible due to the dynamic nature of the OTP.</li><li>Incorrect. OTP systems don’t rely on static passwords. A static password wouldn’t be guessed because OTPs are dynamically generated and change frequently.</li></ul></li><li>B. Someone who finds a one-time password token could log in as the user by guessing the user ID: <ul><li>Knowing the OTP token alone does not provide access unless the attacker also knows the corresponding user ID.</li><li>Review: <strong>Partially plausible</strong>. <ul><li>If the system doesn’t properly validate both the OTP and user ID during the login process (e.g., no secondary password or checks are required), an attacker with access to the OTP could theoretically guess the user ID and gain access.</li><li>However, this relies on a significant flaw in the implementation of the system, which is not a guaranteed assumption in the question.</li></ul></li></ul></li><li>C. Someone who knows the user ID could derive the password: <ul><li>OTPs are generated using algorithms that cannot be reversed to derive the password simply from knowing the user ID.</li><li>Incorrect. OTPs are generated using secure algorithms and cannot be derived from the user ID alone.</li></ul></li></ul></li><li>Mitigations for Replay Attacks: <ul><li>Time-bound OTPs: Ensure OTPs expire quickly, making captured passwords unusable after a brief period.</li><li>Challenge-Response Authentication: Use unique session identifiers or nonces to ensure each authentication session is unique.</li><li>Encrypted Communication: Use protocols like TLS to secure the transmission of user IDs and OTPs, preventing eavesdropping.</li></ul></li></ul></li><li>An organization has configured its applications to utilize an LDAP server for authentication. The organization has set up <ul><li><strong>A.</strong> Automatic sign-on</li><li><strong>B.</strong> LDAP sign-on</li><li><strong>C.</strong> Single sign-on</li><li><strong>D.</strong> Reduced sign-on</li><li>Answer: <strong>D.</strong> Reduced sign-on is the term used to describe an environment where many different systems use a centralized authentication server (such as LDAP).</li><li>D. Reduced sign-on</li><li>Explanation: <ul><li>In this scenario, the organization uses LDAP (Lightweight Directory Access Protocol) as a centralized authentication mechanism for multiple applications. This setup aligns with the concept of Reduced Sign-On (RSO), where: <ul><li>Reduced Credential Management: <ul><li>Users can access multiple systems using the same set of credentials, thanks to the centralized authentication provided by LDAP.</li></ul></li><li>Not True SSO: <ul><li>Although the credentials are centralized, users might still need to log in to each system separately unless additional mechanisms (like session tokens or identity federation) are implemented to enable seamless Single Sign-On (SSO).</li></ul></li><li>Key Difference Between RSO and SSO: <ul><li>RSO reduces the number of credentials users need to remember by centralizing authentication (as in LDAP).</li><li>SSO eliminates the need for repeated logins across systems after the initial authentication, requiring additional configurations or protocols like SAML, OAuth, or Kerberos.</li></ul></li></ul></li></ul></li><li>Why Other Options Are Incorrect: <ul><li>A. Automatic sign-on: <ul><li>Automatic sign-on refers to scenarios where users are logged in automatically without providing credentials (e.g., via stored sessions or local authentication). LDAP does not inherently provide automatic sign-on.</li></ul></li><li>B. LDAP sign-on: <ul><li>While LDAP is being used for authentication, &quot;LDAP sign-on&quot; is not a recognized term describing the concept of centralized authentication.</li></ul></li><li>C. Single sign-on: <ul><li>LDAP alone does not implement true SSO. Users might still need to authenticate separately for each application.</li></ul></li></ul></li></ul></li><li>An organization has hundreds of remote locations containing valuable equipment and needs to enact a secure access control system. The locations do not have electricity. What is the best choice for an access control method that can be implemented at these locations? <ul><li><strong>A.</strong> Keycards</li><li><strong>B.</strong> Metal keys</li><li><strong>C.</strong> Cipher locks</li><li><strong>D.</strong> Video surveillance</li><li>Answer: <strong>C.</strong> The best choice for an access control system for many remote locations is cipher locks. They do not require a power supply or remote connectivity, but they can be configured with a different combination for each user, and some retain a memory of which persons used them.</li><li><strong>C. Cipher Locks Is the Best Choice:</strong><ul><li>Electricity Independence: <ul><li>Cipher locks (mechanical combination locks) do not require a power source, making them practical for remote locations without electricity.</li></ul></li><li>User-Specific Combinations: <ul><li>Unlike metal keys, cipher locks can be programmed with unique codes for different users. This allows for better tracking and control, as codes can be changed if needed without replacing the physical lock.</li></ul></li><li>Security Flexibility: <ul><li>Cipher locks eliminate the risks associated with lost or duplicated keys. If a code is compromised, it can be easily changed.</li></ul></li><li>Optional Tracking Capability: <ul><li>Some cipher locks come with the ability to track which combinations were used, adding an audit trail for accountability, which metal keys cannot provide.</li></ul></li><li>Why Other Options Are Less Suitable: <ul><li>A. Keycards: <ul><li>Keycards typically require electronic readers and power, making them unsuitable for locations without electricity.</li></ul></li><li>B. Metal Keys: <ul><li>While simple and reliable, metal keys cannot provide user-specific access or tracking. Lost or copied keys pose significant risks.</li></ul></li><li>D. Video Surveillance: <ul><li>Surveillance systems require power and only monitor access rather than controlling it directly.</li></ul></li></ul></li></ul></li></ul></li></ol>',7)]))}const c=i(o,[["render",r],["__file","Q.html.vue"]]),u=JSON.parse('{"path":"/308/Q.html","title":"","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"CHAPTER 4 IT Life Cycle Management","slug":"chapter-4-it-life-cycle-management","link":"#chapter-4-it-life-cycle-management","children":[]},{"level":2,"title":"CHAPTER 5 IT Service Management and Continuity","slug":"chapter-5-it-service-management-and-continuity","link":"#chapter-5-it-service-management-and-continuity","children":[]},{"level":2,"title":"CHAPTER 6 Information Asset Protection","slug":"chapter-6-information-asset-protection","link":"#chapter-6-information-asset-protection","children":[]}],"git":{"updatedTime":1733706436000},"filePathRelative":"308/Q.md"}');export{c as comp,u as data};
