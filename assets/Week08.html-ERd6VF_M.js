import{_ as t,c as a,a as i,o as s}from"./app-CIlVrypY.js";const n="/Se/images/306/w0801.png",o="/Se/images/306/w0802.png",r="/Se/images/306/w0803.png",l="/Se/images/306/w0804.png",d="/Se/images/306/w0805.png",h="/Se/images/306/w0806.png",c="/Se/images/306/w0807.png",u="/Se/images/306/w0808.png",p="/Se/images/306/w0809.png",m="/Se/images/306/w0810.png",f={};function y(g,e){return s(),a("div",null,e[0]||(e[0]=[i('<p>CYB306 Cyber-Physical Vehicle System Security​</p><h1 id="chapter-8-human-factors-in-transportation-cyber-physical-systems-a-case-study-of-a-smart-automated-transport-and-retrieval-system-smartatrs-​" tabindex="-1"><a class="header-anchor" href="#chapter-8-human-factors-in-transportation-cyber-physical-systems-a-case-study-of-a-smart-automated-transport-and-retrieval-system-smartatrs-​"><span>Chapter 8: Human Factors in Transportation Cyber-Physical Systems: A Case Study of a Smart Automated Transport and Retrieval System (SmartATRS)​</span></a></h1><h2 id="objectives" tabindex="-1"><a class="header-anchor" href="#objectives"><span>Objectives</span></a></h2><ol><li>Introduction​</li><li>Related human factors approaches​</li><li>Case study​</li><li>Discussion​</li><li>Conclusions and future work​</li></ol><h2 id="_1-introduction​" tabindex="-1"><a class="header-anchor" href="#_1-introduction​"><span>1. Introduction​</span></a></h2><p>Human Factors (HF), also known as ‘ergonomics’, are defined as the scientific discipline concerned with the understanding of interactions amongst humans and other elements of a system.​</p><p>The rationale behind HF is to understand how a system can be designed so that it is suitable for the intended user by complimenting their abilities, as opposed to users adapting to a design that is challenging.​</p><p>To achieve this, it is necessary to understand the variability within the user community, including age, cognitive ability, cultural diversity and physical ability.​</p><p>HF are considered by the UK Ministry of Defense as a systematic systems engineering process known as Human Factors Integration (HFI), which identifies, tracks and resolves human-related considerations, to ensure a balanced development in terms of technologies and human aspects.​</p><p>This concept can be applied during the development of Transportation Cyber-Physical Systems, together with user-centred design, UX and universal design, to ensure that the system meets expectations.​</p><p>There are many examples of cyber-physical systems in the transportation sector where the HF approach has been adopted including aircraft flight decks, air traffic control systems and the design of vehicles.​</p><p>Smart Automated Transport and Retrieval System (SmartATRS) is considered as a Transportation Cyber-Physical System where usability evaluations are conducted to assess the suitability of a system for the user community of people with reduced physical ability.​</p><h2 id="_2-related-human-factors-approaches​" tabindex="-1"><a class="header-anchor" href="#_2-related-human-factors-approaches​"><span>2. Related human factors approaches​</span></a></h2><p>In order to develop SmartATRS, it was necessary to consider aspects of Human Computer Interaction (HCI) in terms of ergonomics of Human System Interaction(HSI), Universal Design and Design for All The ISO standard of ergonomics of HIS was originally recognised as Human-Centred Design (HCD). ​</p><p>It can be defined as an iterative process consisting of five core stages. ​</p><p>The first stage is to understand the context of use in order to generate user requirements. ​</p><p>The requirements are then utilised to produce design solutions that can be evaluated against the user requirements. ​</p><p>The iterative nature of HCD is produced by the involvement of users during the design process, which could lead to modifications to the design of the system.​</p><h3 id="_2-1-human-factors-integration​" tabindex="-1"><a class="header-anchor" href="#_2-1-human-factors-integration​"><span>2.1 Human factors integration​</span></a></h3><p>HFI is considered as a system engineering process that allows the human component of a system to be identified and any human-related aspects that would adversely impact the development to be traded off.​</p><p>The process exists within capability management that concerns the responsibilities of users in capability planning, generation and delivery roles.​</p><p>It is stated that HFI has multiple benefits including minimising errors in the design through thorough correct analysis of HF, resulting in reduced product recalls, development costs, user training and ongoing maintenance costs.​</p><p>HFI consists of a framework containing seven domains to ensure that issues, risks, assumptions, constraints and requirements are captured.​</p><p>These domains are manpower, personnel, training, human factors engineering, system safety, health hazards and social and organizational. ​</p><p>The HFI process can be applied to Transportation Cyber-Physical Systems to provide an evaluation of their integration with the existing processes.​</p><h3 id="_2-2-human-centred-design​" tabindex="-1"><a class="header-anchor" href="#_2-2-human-centred-design​"><span>2.2 Human-centred design​</span></a></h3><p>One of the key principles of the concept is the involvement of potential users during both the design and development of the system.​</p><p>Placing the user at the centre of the design in order to achieve the ISO Standard can be accomplished by adhering to the four recommendations stated by Norman. ​</p><p>It is imperative that a system must be easy to evaluate, the users can determine the actions that are possible at any point in time and the structure of a system must be transparent to users including any conceptual models, alternative actions and the outcomes of actions.​</p><p>Norman’s final recommendation suggests that natural mappings between intentions and required actions should be followed by a system. ​</p><p>Norman also defined seven principles of design that ensures the user is assisted when performing tasks. ​</p><p>These principles include the creation of understandable operating manuals, simplified task structure to avoid memory overloads, planning for user errors to ensure recovery is always possible and ensuring that it is obvious which actions need to be performed to achieve the system goal.​</p><h3 id="_2-3-usability-evaluation​" tabindex="-1"><a class="header-anchor" href="#_2-3-usability-evaluation​"><span>2.3 Usability evaluation​</span></a></h3><p>Usability is defined as the quality of a user’s experience when interacting with products or systems that can be measured in terms of effectiveness, efficiency and satisfaction.​</p><p>A variety of factors can contribute to usability including the ease of learning, memorability, error frequency and intuitive design, which can be evaluated through participative enquiry through the adoption of methods including focus groups, scenarios, surveys and interviews.​</p><p>An alternative strategy to understand the learnability of a system to new or infrequent users is cognitive walk throughs where a series of tasks are conducted from the user’s perspective.​</p><p>The System Usability Scale (SUS) and NASA TLX are two tools that can be adopted to assess intuitive design and the demands experienced by users when interacting.​</p><p>SUS was originally developed by Brooke to provide a ‘quick and dirty’ reliable tool for measuring usability consisting of a 10-item questionnaire with 5 response sections from ‘strongly agree’ to ‘strongly disagree’.​</p><p>NASA TLX is a subjective workload assessment tool that derives an overall usability score based on the subscales of mental demand, physical demand, temporal demand, performance, effort and frustration and thus determines the effect of each interaction modality on the user.​</p><h3 id="_2-4-interaction-modalities​" tabindex="-1"><a class="header-anchor" href="#_2-4-interaction-modalities​"><span>2.4 Interaction modalities​</span></a></h3><p>Traditionally, HCI was considered as unimodal where users can only interact through a single channel, e.g., a keyboard.​</p><p>However, it is multimodal as users interact with a variety of devices such as the keyboard, mouse and display to perform tasks.​</p><p>Multimodal systems was originally defined by Oviatt as systems ‘that process two or more combined user input modes in a coordinated manner with multimedia outputs’.​</p><p>The rationale behind multimodality is to offer alternative channels for users to align with the natural method of interaction in the world (i.e., through the five major senses of sight, hearing, touch, smell and taste).​</p><p>Advances in hardware and software are enabling multimodal systems to emerge where humans are able to interact through natural methods including speech, touch and gesture.​</p><p>The advent of smartphones illustrates multimodal interaction where the device can be operated via a variety of methods.​</p><h2 id="_3-case-study​" tabindex="-1"><a class="header-anchor" href="#_3-case-study​"><span>3. Case study​</span></a></h2><p>SmartATRS can be considered as a Transportation Cyber-Physical System as it enables a user to transport their powered wheelchair in a vehicle. ​</p><p>Furthermore, the system comprises of multiple independently operable constituent systems (e.g., the automated tailgate, platform lift and motorised driver’s seat, as seen in Fig. 8.1) that can only provide the functionality of SmartATRS when combined together as a cyberphysical system. ​</p><p>SmartATRS was developed to control an existing assistive technology called the ATRS, originally developed by Freedom Sciences Inc. ​</p><p>ATRS is a technically advanced system first featured in New Scientist magazine with the objective of creating a reliable, robust means for a wheelchair user to autonomously dock a powerchair onto a platform lift without the need of an assistant. ​</p><p>ATRS requires a standard multipurpose vehicle to be installed with three components; a motorized seat that rotates and exits the vehicle through the driver’s door, an automated tailgate and a platform lift fitted in the rear of the vehicle.​</p><hr><p>Using a joystick attached to the driver’s seat, a user with reduced physical ability manoeuvres the powerchair to the rear of the vehicle until it is adjacent to the lift and within line of sight of two highly reflective fiducials. ​</p><p>On an input from the user (via a button press), a laser guidance system comprising a compact Light Detection and Ranging (LiDAR) unit coupled with robotics fitted to powerchair, locates the exact position of the lift and proceeds to autonomously drive the powerchair onto the platform, as shown in Fig. 8.1.​</p><p><img src="'+n+'" alt="image-20241104224004824"></p><p>Figure 8.1 ATRS operating zones.</p><p><img src="'+o+'" alt="image-20241104224028687"></p><p>Figure 8.2 Wireless keyfobs used to control ATRS.</p><h3 id="_3-1-requirements" tabindex="-1"><a class="header-anchor" href="#_3-1-requirements"><span>3.1 Requirements</span></a></h3><ul><li>SmartATRS was developed to provide the exact functionality of the keyfobs on a smartphone interface.</li><li>Demonstrations of ATRS to users with reduced physical abilities were performed at the 2011 Mobility Roadshow, a UK consumer-based event, showcasing mobility products and innovation.</li><li>Based on the demonstrations at the Mobility Roadshow, requirements were defined for SmartATRS using Volere Requirements Shells and categorised in terms of Functionality (FR), Interoperability (IR), Maintainability (MR), Performance (PR), Portability (PTR), Reliability (RR), Safety (SFR) and Usability (UR). The defined requirements were as follows: <ol><li>(SFR1) SmartATRS shall not prevent ATRS from being operated by the handheld pendants or keyfobs.​</li><li>(FR1) SmartATRS shall be able to control the following functions: the Freedom Seat, Tracker Lift and Automated Tailgate.​</li><li>(SFR2) SmartATRS shall ensure safe operation of all ATRS functions.​</li><li>(UR1) The user interface of SmartATRS shall be created in a design that a user with reduced finger dexterity would be able to use.​</li><li>(RR1) SmartATRS shall be reliable, as a user would depend on the system for their independence.​</li><li>(FR2) ATRS shall still function as if being operated by the handheld pendants and keyfobs.​</li><li>(PR1) SmartATRS shall minimise any additional delay to the functioning of ATRS.​</li><li>(MR1) SmartATRS shall be easy to configure by installers.​</li><li>(MR2) SmartATRS shall be easy to install into a standard ATRS.​</li><li>(PTR1) SmartATRS shall be compatible with all popular smartphone operating systems that have web browsers and customisable voice control.​</li></ol></li></ul><h3 id="_3-2-system-architecture" tabindex="-1"><a class="header-anchor" href="#_3-2-system-architecture"><span>3.2 System architecture</span></a></h3><ul><li>SmartATRS was originally developed with two interaction methods (touch and joystick), but this was subsequently increased through the incorporation of head and smartglass-based interaction modalities.</li><li>Fig. 8.3 shows the system architecture diagram for SmartATRS including the integrated existing ATRS components, as well as the component and user interactions.</li></ul><p><img src="'+r+'" alt="image-20241104224520040"></p><p>Figure 8.3 SmartATRS system architecture diagram. Component interactions indicated by black and yellow lines (light grey lines in print versions) and the user interactions illustrated in red (dark grey in print versions).</p><hr><ul><li>In the standard ATRS, keyfobs and handheld pendants were the only interaction methods, whereas with SmartATRS, the original interaction methods are touch or joystick based.</li><li>Junction boxes were manufactured to retain the operation of the existing handheld pendants as a backup method.</li><li>As all of the ATRS components contained relays, a relay board comprising an embedded web server was used to interface between the components and JavaScript.</li><li>The server stored the HTML and JavaScript Graphical User Interfaces (GUI) as web pages and JavaScript XMLHttp Requests (objects that transfer data between a web browser and server were transmitted to access an Extensible Markup Language (XML) file).</li><li>The file contains the timer durations for each ATRS function denoted as integers that represented the number of milliseconds that each function was switched on for.</li><li>An XML editor was used to view and change the timer durations, therefore ensuring that the process was not visible to end users.</li></ul><h3 id="_3-3-user-interface-design" tabindex="-1"><a class="header-anchor" href="#_3-3-user-interface-design"><span>3.3 User interface design</span></a></h3><ul><li>The SmartATRS user interface (shown in Fig. 8.4) was developed based upon the views of users at the 2011 Mobility Roadshow and incorporated user feedback and safety features that were not present in the keyfobs.</li><li>Seven command buttons were used to activate each ATRS function.</li><li>The red emergency stop button was twice the width of the other buttons, so that it could be selected quickly in an emergency situation.</li><li>The large buttons reduced the risk of incorrect selection by users with reduced finger dexterity.</li><li>The command button changed colour depending on the current state of SmartATRS, with blue representing currently operating features and orange to represent a disabled function.</li></ul><p><img src="'+l+'" alt="image-20241104224653051"></p><p>Figure 8.4 SmartATRS user interface.</p><h3 id="_3-4-risk-analysis​" tabindex="-1"><a class="header-anchor" href="#_3-4-risk-analysis​"><span>3.4 Risk analysis​</span></a></h3><ul><li>As identified in the fifth systems safety domain of the HFI process, it is important to consider risks in order to achieve a user-centred design.</li><li>Transportation Cyber-Physical Systems can present multiple risks to users due to motorised physical components.</li><li>Therefore, the SmartATRS case study was used to establish potential risks that can exist with Transportation Cyber-Physical Systems technologies in a three-stage risk analysis framework for System of Systems (SoS), consisting of threat identification, risk analysis and risk evaluation.</li><li>In order to identify risks, an in-depth understanding of the system’s structure needs to be established in terms of threat sources and vulnerable system elements.</li><li>This results in identification of risks that are present within the system environment.</li></ul><p>Table 8.1 Likelihood and Impacts of SmartATRS Risks</p><table><thead><tr><th>ID</th><th>Identified Risk</th><th>Likelihood (L, M, H)</th><th>Impact on Systems</th><th>Impact on Interoperability</th><th>Impact Level (L, M, H)</th><th>Risk Level (L, M, H)</th></tr></thead><tbody><tr><td>S1</td><td>Smartphone must be in range of the router for Wi-Fi to be accessible.</td><td>L</td><td>Wi-Fi connection will not be available for smartphone. The system cannot be used.</td><td>The smartphone will not be able to connect and communicate with other systems.</td><td>H</td><td>M</td></tr><tr><td>S2</td><td>Vehicle cannot receive commands if the smartphone is not available.</td><td>M</td><td>The system cannot be operated without the smartphone.</td><td>System cannot operate.</td><td>H</td><td>H</td></tr></tbody></table><p>L, Low; M, Medium; H, High.</p><h3 id="_3-5-task-analysis-usability-evaluations-and-workload-measurements​" tabindex="-1"><a class="header-anchor" href="#_3-5-task-analysis-usability-evaluations-and-workload-measurements​"><span>3.5 Task analysis, usability, evaluations and workload measurements​</span></a></h3><ul><li>Controlled usability evaluations were performed involving the user community to obtain an accurate assessment of usability of varying modalities.</li><li>Prior to the conduction of the evaluations, it was necessary to perform a Hierarchical Task Analysis (HTA) to obtain an understanding of the tasks involved with operating SmartATRS.</li></ul><h4 id="_3-5-1-hierarchical-task-analysis" tabindex="-1"><a class="header-anchor" href="#_3-5-1-hierarchical-task-analysis"><span>3.5.1 Hierarchical task analysis</span></a></h4><ul><li>Adopting HTA enabled the tasks to be performed in the controlled usability evaluations to be determined.</li><li>This was achieved by deconstructing the high-level parent task (i.e., departing or arriving in a vehicle) into subtasks by using a numbering system in a hierarchical structure, as shown in the extract in Fig. 8.5.</li><li>The SmartATRS HTA for departing in the vehicle consisted of six subtasks: (1) preparing vehicle, (2) activating lift and seat out of vehicle, (3) preparing powerchair, (4) autonomous docking, (5) activating lift and seat into vehicle and (6) departure.</li><li>These tasks needed to be performed sequentially in order to successfully depart in the vehicle with the powerchair safely stowed.</li><li>The addition of screenshots of SmartATRS to the HTA highlighted the tasks currently supported by smartphone interaction.</li></ul><p><img src="'+d+'" alt="image-20241104225244006"></p><p>Figure 8.5 An extract of the SmartATRS Hierarchical Task Analysis for departing in a vehicle.</p><h4 id="_3-5-2-evaluation-1-keyfob-touch-and-joystick-based-interactions" tabindex="-1"><a class="header-anchor" href="#_3-5-2-evaluation-1-keyfob-touch-and-joystick-based-interactions"><span>3.5.2 Evaluation 1 (keyfob, touch and joystick based interactions)</span></a></h4><ul><li>The first controlled usability evaluation was conducted to assess the usability of the interaction methods: keyfobs, touch and joystick.</li><li>The evaluation also provideda means to verify the GUI design of SmartATRS to ensure that it was ‘fit for purpose’.</li><li>The participants of the evaluation (consisting of eight males and four females between the ages of 20 and 60) operated an ATRS-equipped vehicle in an outdoor environment.</li><li>The outdoor aspect of the evaluation inherently produced safety implications for participants who were unfamiliar with operating ATRS.</li><li>In the subsequent evaluations, a simulation was utilised.</li><li>The participants were given a briefing prior to the commencement of the evaluation, consisting of an introduction to ATRS and SmartATRS, the purpose of the evaluation and the expectations of the participants.</li><li>There was an opportunity for questions to be asked.</li><li>The participants performed a series of six tasks using keyfob, touch and joystick based interactions, before completing a questionnaire pack concerning the usability of the methods.</li></ul><h4 id="_3-5-3-evaluation-1-results" tabindex="-1"><a class="header-anchor" href="#_3-5-3-evaluation-1-results"><span>3.5.3 Evaluation 1 results</span></a></h4><ul><li>SUS: Analysis using the Adjective Rating Scale revealed that keyfob interaction achieved a score of 50.5 (‘Poor Usability’), whereas touch based achieved 81.3 (‘Good Usability’) and interaction using the joystick achieved 63.8 (‘OK Usability’).</li><li>This clearly highlighted that touch interaction was the most usable, with most participants finding keyfob-based interaction challenging.</li><li>One of the most important results highlighted the safety of the emergency stop function and was found when 100% of participants agreed that it was safe using SmartATRS, compared with only 33% using the keyfobs.</li><li>This result was supported by the results from emergency stop times for the keyfobs and touch-based interaction.</li></ul><hr><ul><li>NASA TLX: The box plots in Figs 8.6 and 8.7 provide an example of the comparison of the workload experienced when using keyfob, touch and joystick-based interactions.</li><li>Fig. 8.7 illustrates the differences in the workload experienced between interaction methods and show the minimum, lower-quartile, median, upper-quartile and maximum values.</li><li>It can be seen that touch- based had a significantly lower workload level in all workload types than the keyfobs.</li><li>There are greater mental and physical demands with keyfobs than touch-based interactions.</li><li>As there is an increased likelihood of not successfully accomplishing the tasks with keyfobs, it was found that the temporal demand was higher, whereas with touch-based interactions, there was a low temporal demand as there is an improved chance of accomplishing tasks successfully.</li></ul><p><img src="'+h+'" alt="image-20241104225437015"></p><p>Figure 8.6 Comparing (A) Mental and (B) Physical Demand experienced.</p><p><img src="'+c+'" alt="image-20241104225453866"></p><p>Figure 8.7 Comparing (A) Effort and (B) Frustration experienced.</p><h4 id="_3-5-4-evaluation-2-touch-and-head-based-interactions" tabindex="-1"><a class="header-anchor" href="#_3-5-4-evaluation-2-touch-and-head-based-interactions"><span>3.5.4 Evaluation 2 (touch and head based interactions)</span></a></h4><ul><li>The purpose of the second control usability evaluation was to compare touch and head based interaction modalities through a simulation of SmartATRS.</li><li>These simulations consisted of a relay board with an embedded web server (identical to the relay board located in the vehicle), smartphone, Windows laptop and a projector.</li><li>The web server on the relay board was connected to a wireless LAN module so that a smartphone could connect to the relay board wirelessly.</li><li>The same user interface for SmartATRS existed in the simulation with the relays being operated from the JavaScript, but the relays were not connected to any functions.</li><li>A Windows laptop also connected to the relay board wirelessly and executed JavaScript code that continuously monitored the state on the relays.</li></ul><p><img src="'+u+'" alt="image-20241104225643269"></p><p>Figure 8.8 SmartATRS simulation interface.</p><h4 id="_3-5-5-evaluation-2-results" tabindex="-1"><a class="header-anchor" href="#_3-5-5-evaluation-2-results"><span>3.5.5 Evaluation 2 results</span></a></h4><ul><li><strong>SUS:</strong> Analysis using the Adjective Rating Scale revealed that touch-based interaction achieved a score of 75.7 (‘Good Usability’), whereas head-based achieved 36.7 (‘Poor Usability’). This clearly highlighted that touch interaction was the most usable, with most participants finding interaction with the head challenging.</li><li>A second important result identified the safety of the emergency stop function with each interaction method.</li><li>The results revealed a standard deviation of 4 s for the fingers compared to 14 s for head tracking.</li><li>The average stopping times were 4 and 16 s, respectively.</li><li>The dramatically increased stop times for head tracking were observed to be the time taken to navigate to the emergency stop button using switch control, indicating that using the head is more unpredictable than fingers.</li></ul><hr><ul><li><strong>NASA TLX</strong>: The box plot comparisons in Fig. 8.9 illustrate the differences in the workload experienced between touch and head-based interaction.</li><li>From the minimum, lower-quartile, median, upper-quartile and maximum values, it is evident that ‘fingers’ showed lower mental and temporal demands, thus proving that head tracking was more mentally demanding and stressful to complete efficiently.</li><li>A second important observation was the considerably higher physical demand for head tracking resulting in 65% of participants either not being able to sufficiently use switch control at all or finding it extremely challenging.</li><li>The remaining 35% of participants experienced low workload levels when using the head due to having full range of neck movement.</li><li>The limitations of head tracking are also reflected by the increased effort and frustration levels compared to ‘fingers’.</li><li>Overall the box plots were fairly conclusive that in this particular instance, touch-based interaction was more effective than head interaction.</li></ul><p><img src="'+p+'" alt="image-20241104225817183"></p><p>Figure 8.9 Box plot comparison of NASA Task Load Index (TLX) results in terms of (A) Physical Demand, (B) Mental Demand, (C) Effort and (D) Frustration.</p><h4 id="_3-5-6-evaluation-3-touch-and-smartglass-based-interaction" tabindex="-1"><a class="header-anchor" href="#_3-5-6-evaluation-3-touch-and-smartglass-based-interaction"><span>3.5.6 Evaluation 3 (touch and smartglass based interaction)</span></a></h4><ul><li>The third evaluation compared touch-based and smartglass interaction mediums to ascertain whether smartglasses could potentially be useful for people with reduced physical ability.</li><li>The evaluation was conducted using the Recon Jet smartglass with participants at the 2016 Mobility Roadshow.</li><li>The simulation of SmartATRS, used for Evaluation 2, was applied to this evaluation to eliminate the use of a vehicle and the ATRS components.</li><li>It was necessary to develop a separate SmartATRS user interface for the Recon Jet due to the small display size, as shown in Fig. 8.10.</li><li>The interface retained an identical layout as the smartphone interface, but the buttons were reduced in size so that all could be viewed on the single screen.</li><li>To enable navigation using the touchpad on the Recon Jet, additional JavaScript code was developed that converted the American Standard Code for Information Interchange (ASCII) codes produced from the touchpad into onfocus events.</li><li>Due to the poor usability of the Recon Jet previously established during a feasibility trial, it was therefore decided not to conduct a controlled usability evaluation; therefore no statistical results were obtained.</li></ul><hr><ul><li>The participants for the evaluation had varying physical conditions (including cerebral palsy, spina bifida, arthritis and poliomyelitis) resulting in the use of manual or powered wheelchairs.</li><li>The Recon Jet was integrated into the SmartATRS network in order for the user interface to be displayed and participants were invited to wear the smartglass to ascertain whether they could read the display.</li><li>If it was readable, the participants were instructed on the operation of the smartglass touchpad and buttons, as well as the functionality of the simulation.</li></ul><p><img src="'+m+'" alt="image-20241104225944915"></p><p>Figure 8.10 SmartATRS user interface for Recon Jet.</p><h4 id="_3-5-7-evaluation-3-results" tabindex="-1"><a class="header-anchor" href="#_3-5-7-evaluation-3-results"><span>3.5.7 Evaluation 3 results</span></a></h4><ul><li>The majority of the user group experienced challenges to position the smartglass on their heads due to insufficient dexterity.</li><li>Further challenges were caused by the small text on the user interface resulting in the button names being unreadable and participants were consequently unable to conduct the evaluation.</li><li>A further difficulty for most participants was the small buttons on the device that required significant dexterity to operate.</li><li>The overall result of the evaluation was that the Recon Jet was unsuitable for use as an alternative modality for SmartATRS.</li></ul><h2 id="discussion" tabindex="-1"><a class="header-anchor" href="#discussion"><span>Discussion</span></a></h2><ul><li>The case study focused on SmartATRS that controls ATRS. This is an example of a Transportation Cyber-Physical System that consists of constituent systems that interact in order to transport a powerchair in a vehicle.</li><li>SmartATRS was developed to provide an alternative modality of interaction for ATRS to replace the existing keyfobs, which were challenging for people with reduced finger dexterity to operate.</li><li>The system was centred around a relay board with an embedded web server that interfaced with the ATRS functions.</li><li>This produced a solution that was smartphone independent due to the user interface being accessible from any Wi-Fieenabled device.</li><li>The importance of safety in cyber-physical systems was the rationale behind the development of a risk analysis framework for SoS.</li><li>The framework considered at the three key elements of risk being HSI, interoperability analysis and emergent behaviour.</li><li>The usability of the modalities of interaction for SmartATRS was measured through the conduction of controlled usability evaluations that compared keyfob, touch, head and smartglass based interaction.</li></ul><h2 id="conclusions-and-future-work" tabindex="-1"><a class="header-anchor" href="#conclusions-and-future-work"><span>Conclusions and future work</span></a></h2><ul><li>The knowledge obtained through the development of SmartATRS can be used to generate future directions for Transportation Cyber-Physical Systems.</li><li>The integration of technologies to provide different interaction modalities could improve the usability and user-centred design for people with reduced physical ability.</li><li>Alternative interaction modalities that could be investigated include air gesture, electroencephalogram, head and eye tracking.</li><li>These would provide methods of interaction for users who do not possess the required dexterity to interact through traditional touch-based mediums.</li><li>SmartATRS is presented as a cyber-physical system that provides transportation of a powerchair in a vehicle and consists of a number of constituent systems including the original ATRS components of a motorised driver seat, automated tailgate, platform lift and a LiDAR unit that provides the autonomous docking of the powerchair.</li><li>To assess the usability of SmartATRS to align with HCD, three control usability evaluations were conducted to compare the interaction modalities of keyfobs, touch, joystick, head and smartglasses.</li><li>Future evaluations are planned to assess the possibilities of other technologies being integrated into the SmartATRS cyber-physical system.</li><li>It will be essential to determine whether the technologies retain the safety and existing functionality of the system to enable transportation of a powerchair in a vehicle.</li></ul>',114)]))}const v=t(f,[["render",y],["__file","Week08.html.vue"]]),w=JSON.parse('{"path":"/306/Week08.html","title":"Chapter 8: Human Factors in Transportation Cyber-Physical Systems: A Case Study of a Smart Automated Transport and Retrieval System (SmartATRS)​","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"Objectives","slug":"objectives","link":"#objectives","children":[]},{"level":2,"title":"1. Introduction​","slug":"_1-introduction​","link":"#_1-introduction​","children":[]},{"level":2,"title":"2. Related human factors approaches​","slug":"_2-related-human-factors-approaches​","link":"#_2-related-human-factors-approaches​","children":[{"level":3,"title":"2.1 Human factors integration​","slug":"_2-1-human-factors-integration​","link":"#_2-1-human-factors-integration​","children":[]},{"level":3,"title":"2.2 Human-centred design​","slug":"_2-2-human-centred-design​","link":"#_2-2-human-centred-design​","children":[]},{"level":3,"title":"2.3 Usability evaluation​","slug":"_2-3-usability-evaluation​","link":"#_2-3-usability-evaluation​","children":[]},{"level":3,"title":"2.4 Interaction modalities​","slug":"_2-4-interaction-modalities​","link":"#_2-4-interaction-modalities​","children":[]}]},{"level":2,"title":"3. Case study​","slug":"_3-case-study​","link":"#_3-case-study​","children":[{"level":3,"title":"3.1 Requirements","slug":"_3-1-requirements","link":"#_3-1-requirements","children":[]},{"level":3,"title":"3.2 System architecture","slug":"_3-2-system-architecture","link":"#_3-2-system-architecture","children":[]},{"level":3,"title":"3.3 User interface design","slug":"_3-3-user-interface-design","link":"#_3-3-user-interface-design","children":[]},{"level":3,"title":"3.4 Risk analysis​","slug":"_3-4-risk-analysis​","link":"#_3-4-risk-analysis​","children":[]},{"level":3,"title":"3.5 Task analysis, usability, evaluations and workload measurements​","slug":"_3-5-task-analysis-usability-evaluations-and-workload-measurements​","link":"#_3-5-task-analysis-usability-evaluations-and-workload-measurements​","children":[]}]},{"level":2,"title":"Discussion","slug":"discussion","link":"#discussion","children":[]},{"level":2,"title":"Conclusions and future work","slug":"conclusions-and-future-work","link":"#conclusions-and-future-work","children":[]}],"git":{"updatedTime":1730781819000,"contributors":[{"name":"rhyme_yang","email":"rhyme_yang@live.cn","commits":1}]},"filePathRelative":"306/Week08.md"}');export{v as comp,w as data};
