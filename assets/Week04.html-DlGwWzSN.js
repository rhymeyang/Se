import{_ as t,c as i,a as n,o}from"./app-UY2AS4Ti.js";const a="/Se/images/306/25.png",s="/Se/images/306/26.png",r="/Se/images/306/27.png",l="/Se/images/306/28.png",c="/Se/images/306/29.png",h="/Se/images/306/30.png",d="/Se/images/306/31.png",m="/Se/images/306/32.png",u={};function p(f,e){return o(),i("div",null,e[0]||(e[0]=[n('<h1 id="chapter-4-real-time-control-systems" tabindex="-1"><a class="header-anchor" href="#chapter-4-real-time-control-systems"><span>Chapter 4: Real-time control systems</span></a></h1><h2 id="understanding-control-system" tabindex="-1"><a class="header-anchor" href="#understanding-control-system"><span>UNDERSTANDING CONTROL SYSTEM</span></a></h2><p>https://www.youtube.com/watch?v=2BwUMk10WqI</p><h2 id="objectives" tabindex="-1"><a class="header-anchor" href="#objectives"><span>OBJECTIVES</span></a></h2><ol><li>Introduction</li><li>Components in real-time control systems</li><li>Real-time control systems in autonomous vehicles</li><li>Conclusions and future directions</li></ol><h3 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction"><span>1. INTRODUCTION</span></a></h3><ul><li>A control system consists of a set of devices that senses, alters or modulates the behaviours or tunable parameters of the controlled plant such that the desirable states of the plant are achieved.</li><li>In a vehicle, a control system can range from the temperature control of the air conditioner to the stability control of the vehicle motion.</li><li>Advanced real-time control systems interpret and analyse sensory information to identify appropriate navigation paths to the desired destination, as well as obstacles and relevant signage, ensuring safe and efficient travel.</li></ul><h3 id="_2-components-in-real-time-control-systems" tabindex="-1"><a class="header-anchor" href="#_2-components-in-real-time-control-systems"><span>2. COMPONENTS IN REAL-TIME CONTROL SYSTEMS:</span></a></h3><h4 id="_2-1-typical-real-time-control-system" tabindex="-1"><a class="header-anchor" href="#_2-1-typical-real-time-control-system"><span>2.1 TYPICAL REAL-TIME CONTROL SYSTEM</span></a></h4><ul><li>A real-time control system is a control system that ‘controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time’.</li><li>In a control system, ‘real-time’ means the programs run by the controller must be fast enough to generate response within specified time limits, often referred to as ‘deadlines’.</li><li>Such limits or constraints are always in the order of milliseconds or even microseconds.</li><li>For a real-time controller, the hardware scan cycle must be short enough, and the program should be well designed in order that the input scan, program executing and output scan can be completed within a scan cycle.</li></ul><h4 id="_2-1-1-smart-traffic-light-control-systems" tabindex="-1"><a class="header-anchor" href="#_2-1-1-smart-traffic-light-control-systems"><span>2.1.1 SMART TRAFFIC LIGHT CONTROL SYSTEMS</span></a></h4><ul><li>Smart traffic lights (or intelligent traffic lights) form a real- time traffic control system by combining traditional traffic lights with an intelligent signal control system.</li><li>The signal control system uses an array of sensors to detect traffic density and adjusts traffic lights via artificial intelligence to increase traffic efficiency and safety.</li></ul><h4 id="_2-1-2-autopilot-systems" tabindex="-1"><a class="header-anchor" href="#_2-1-2-autopilot-systems"><span>2.1.2 AUTOPILOT SYSTEMS</span></a></h4><ul><li>An aircraft autopilot (automatic pilot) system controls the aircraft without the pilot directly operating the controls.</li><li>Such system is developed to reduce the work load of human pilots in order to lessen their fatigue and reduce operation errors during long flights.</li><li>It handles most of the time-intensive nonedecision-making tasks, helping the human pilots to focus on the overall status of the aircraft and flight.</li><li>The control software reads the aircraft’s current speed, pose, height and location and then issues control signal to a flight control system, which is a lower-level actuator controller, to adjust the control surfaces of the aircraft in order to maintain the aircraft’s attitude, height and speed while guaranteeing the lateral, vertical and longitudinal stability.</li></ul><p><img src="'+a+'" alt="image-20241005234939751"></p><p>Figure 4.2 An aircraft autopilot system.</p><h2 id="_2-2-structure-of-the-real-time-control-system-of-autonomous-vehicles" tabindex="-1"><a class="header-anchor" href="#_2-2-structure-of-the-real-time-control-system-of-autonomous-vehicles"><span>2.2 STRUCTURE OF THE REAL-TIME CONTROL SYSTEM OF AUTONOMOUS VEHICLES</span></a></h2><ul><li>The implementation of real-time control for an autonomous vehicle relies on a series of components, and each of these components performs a different role.</li><li>Some of them act as the ‘brain’ to coordinate and make decisions and we call them ECUs; some of them are responsible for collecting the external and internal information and we call them sensors; some of them are responsible for executing commands from ECUs and we call them actuators; some of them play as information-transmitting channels and we call them communication bus.</li></ul><p><img src="'+s+'" alt="image-20241005235411246"></p><p>Figure 4.3 Components in real-time control system for an autonomous vehicle. ECUs, elec- tronic control units.</p><h2 id="_2-3-electronic-control-units" tabindex="-1"><a class="header-anchor" href="#_2-3-electronic-control-units"><span>2.3 ELECTRONIC CONTROL UNITS</span></a></h2><ul><li>Electric Control Unit (ECU) refers to dedicated vehicular embedded microcontrollers that controls one or more of the electrical systems or subsystems in a vehicle.</li><li>Types of ECU include central control module, engine control module, powertrain control module, transmission control module, electronic brake control module, speed control unit, body control module, suspension control module, humanemachine interface, telematic control unit, brake control module (including Anti-lock Braking System (ABS) and Electronic Stability Control (ESC)), battery management system, door control unit and seat control unit. Some modern motor vehicles have up to 80 ECUs.</li></ul><h2 id="_2-4-sensors-in-autonomous-vehicles" tabindex="-1"><a class="header-anchor" href="#_2-4-sensors-in-autonomous-vehicles"><span>2.4 SENSORS IN AUTONOMOUS VEHICLES</span></a></h2><p><img src="'+r+'" alt="image-20241005235703644"></p><p>Figure 4.4 Sensors in an autonomous vehicle.</p><h3 id="_2-4-1-ultrasonic-sensors" tabindex="-1"><a class="header-anchor" href="#_2-4-1-ultrasonic-sensors"><span>2.4.1 ULTRASONIC SENSORS</span></a></h3><ul><li>An ultrasonic sensor is a device that can measure the distance by using ultrasonic waves.</li><li>It works by sending out an ultrasonic wave at a specific frequency and receiving the wave reflected back from the target.</li></ul><p><img src="'+l+'" alt="Figure 4.5 Ultrasonic sensor."> Figure 4.5 Ultrasonic sensor.</p><h4 id="_2-4-2-3-different-uses-of-radio-detection-and-ranging-sensor-in-autonomous-vehicles" tabindex="-1"><a class="header-anchor" href="#_2-4-2-3-different-uses-of-radio-detection-and-ranging-sensor-in-autonomous-vehicles"><span>2.4.2.3 DIFFERENT USES OF RADIO DETECTION AND RANGING SENSOR IN AUTONOMOUS VEHICLES</span></a></h4><p>Table 4.1 The Different Applications for Radio Detection and Ranging Sensors in Autonomous Vehicles</p><table><thead><tr><th>Application</th><th>Detection Range</th><th>Field of View</th><th>Technology</th></tr></thead><tbody><tr><td>Adaptive cruise control (ACC)</td><td>150 ~ 200 m</td><td>+-10 ~ 20 degrees</td><td>Single beam, 24 GHz</td></tr><tr><td>Forward collision warning and precrash detection</td><td>40 ~ 90 m</td><td>+-35 ~ 45 degrees</td><td>Single beam,76 GHz/24 GHz</td></tr><tr><td>Blind spot detection, lane change assist and cross-traffic detection</td><td>30 ~ 40 m</td><td>+-40 ~ 50 degrees</td><td>Single beam, 76 GHz/24 GHz</td></tr><tr><td>ACC with stop and go</td><td>Multiple ranges</td><td>Multiple ranges</td><td>Multimode electronically scan</td></tr></tbody></table><h4 id="_2-4-6-2-real-time-kinematic-global-positioning-system" tabindex="-1"><a class="header-anchor" href="#_2-4-6-2-real-time-kinematic-global-positioning-system"><span>2.4.6.2 REAL TIME KINEMATIC GLOBAL POSITIONING SYSTEM</span></a></h4><ul><li>RTK (real time kinematic) satellite navigation is a technique used to enhance the precision of position data obtained from standard GPS systems.</li><li>The accuracy of the GPS may be affected by many contributing factors like clock difference between satellites, refraction of electromagnetic waves in ionosphere, random noise added to clock in commercialised GPS signal, not enough satellites (less than 8 w 9), and signal bounced off buildings or canyons.</li><li>A good way to correct these variable errors is to set up a fixed GPS receiver as a base station whose position can be measured.</li></ul><h4 id="_2-4-6-2-real-time-kinematic-global-positioning-system-1" tabindex="-1"><a class="header-anchor" href="#_2-4-6-2-real-time-kinematic-global-positioning-system-1"><span>2.4.6.2 REAL TIME KINEMATIC GLOBAL POSITIONING SYSTEM</span></a></h4><p><img src="'+c+'" alt="image-20241006000632581"></p><p>Figure 4.11 Real time kinematic global positioning system (GPS).</p><h2 id="_2-5-actuators" tabindex="-1"><a class="header-anchor" href="#_2-5-actuators"><span>2.5 ACTUATORS</span></a></h2><ul><li>For any control system, the actuators are the terminal components which are responsible for moving or controlling the system.</li><li>In modern vehicles, there are various actuators like motors, valves and hydraulic cylinders.</li><li>These actuators act as ‘movers’ to execute the steering, gear changing, braking, etc.</li></ul><h2 id="_3-real-time-control-systems-in-autonomous-vehicles" tabindex="-1"><a class="header-anchor" href="#_3-real-time-control-systems-in-autonomous-vehicles"><span>3. REAL-TIME CONTROL SYSTEMS IN AUTONOMOUS VEHICLES</span></a></h2><p><img src="'+h+'" alt="image-20241006001637075"></p><p>Figure 4.14 General structure of autonomous vehicle real-time control system.</p><h3 id="_3-1-2-localisation" tabindex="-1"><a class="header-anchor" href="#_3-1-2-localisation"><span>3.1.2 LOCALISATION</span></a></h3><ul><li>The localisation for autonomous vehicles includes tasks at three different levels: road-level localisation, lane-level localisation and feature-level localisation.</li><li>The road-level localisation provides the rough estimation of the vehicle position in an existing road map.</li><li>This level of localisation can be achieved by utilizing the position information provided by a GPS system.</li><li>However, the refreshing rate of civil-level GPS devices (normally lower than 1 Hz) and the accuracy (~3 m) are not sufficient for autonomous driving.</li><li>To achieve lane-level localisation, sensory information from IMUs and wheel speed sensors needs to be fused together with the raw position information to provide higher-positioning refreshing rate (&gt;20 Hz) and accuracy (&lt;0.5 m).</li><li>Combining this fused position information with an existing road lane map, the vehicle’s current lane selection on a road can be identified.</li></ul><h3 id="_3-3-4-neural-networkebase-d-advanced-control" tabindex="-1"><a class="header-anchor" href="#_3-3-4-neural-networkebase-d-advanced-control"><span>3.3.4 NEURAL NETWORKEBASE D ADVANCED CONTROL</span></a></h3><ul><li>Besides the traditional control models mentioned above, there are new motion plan and control methods emerging in the past two decades.</li><li>These methods can have more complex behaviour patterns and better performance.</li><li>In NN-based control model, an NN is used to control the motion of the vehicle. The NN needs to be trained with human driving data before it can be used to control the vehicle. A trained NN will mimic the human driver’s behaviour in the training data.</li><li>The input to an NN can be the same as the analytical-based approach, namely, extracted features from the perception modules</li><li>The input to an NN can also be the raw sensory data, for example, the video frames from a camera. In this case a deep NN, which is normally a CNN, needs to be adopted.</li></ul><h3 id="_3-3-4-neural-network-based-advanced-control" tabindex="-1"><a class="header-anchor" href="#_3-3-4-neural-network-based-advanced-control"><span>3.3.4 NEURAL NETWORK-BASED ADVANCED CONTROL</span></a></h3><p><img src="'+d+'" alt="image-20241006005553857"> Figure 4.19 Typical architecture of a Neural Network.</p><h3 id="_3-3-6-model-predictive-control" tabindex="-1"><a class="header-anchor" href="#_3-3-6-model-predictive-control"><span>3.3.6 MODEL PREDICTIVE CONTROL</span></a></h3><p><img src="'+m+'" alt="image-20241006005711604"></p><p>Figure 4.21 The structure of a hierarchical model predictive control (MPC).</p><h2 id="_3-4-autonomous-vehicle-collaboration-in-transportation-cyber-physical-system" tabindex="-1"><a class="header-anchor" href="#_3-4-autonomous-vehicle-collaboration-in-transportation-cyber-physical-system"><span>3.4 AUTONOMOUS VEHICLE COLLABORATION IN TRANSPORTATION CYBER-PHYSICAL SYSTEM</span></a></h2><ul><li>In the scenario where level 5 fully autonomous driving is achieved, autonomous vehicles will not function as independent individuals but rather form a cyber-physical network where all vehicles and infrastructures are connected.</li><li>Under this situation, the motion planning and control system may still work similarly, as today, but the perception and mission planning systems will encounter a big reform.</li><li>The basic methods will not be changed, but the scale and accuracy of the surrounding map built by the perception system will be improved a lot with the help of excessive information exchanged between vehicles and infrastructures.</li></ul><h4 id="_3-4-1-communication-in-transportation-cyber-physical-system" tabindex="-1"><a class="header-anchor" href="#_3-4-1-communication-in-transportation-cyber-physical-system"><span>3.4.1 COMMUNICATION IN TRANSPORTATION CYBER-PHYSICAL SYSTEM</span></a></h4><p>To form the cyber-physical network of vehicles and infrastructures, special communication methods need to be adopted.</p><p>Unlike normal networks, vehicles are always moving around and causing the structure of the network to change.</p><p>The first challenge lies in the communication quality between vehicles and infrastructures.</p><p>Although there is a dedicated short-range communication system designed for this application, more problems still need to be solved.</p><p>For instance, when traffic is heavy, a large number of vehicles may transmit information at the same time and that can interfere with each other’s communication.</p><p>The second challenge lies in the structure of the network.</p><p>It is not realistic to include all vehicles and infrastructures under the same network all the time due to the dynamics in the network; An effective method is grouping the nearby vehicles and infrastructures under a low- level network.</p><h3 id="_3-4-2-vehicle-to-vehicle-collaboration" tabindex="-1"><a class="header-anchor" href="#_3-4-2-vehicle-to-vehicle-collaboration"><span>3.4.2 VEHICLE TO VEHICLE COLLABORATION</span></a></h3><p>With the communication system ready, the collaboration between vehicles can be performed by conducting mission planning based on shared perception information between vehicles.</p><p>This type of collaboration can be very helpful under both highway and urban driving scenarios.</p><p>With extra perception information, mission planning system can plan more ahead for potential dangers.</p><p>Moreover, by sharing mission plan information between each other, a vehicle can learn not only the behaviours but also the reasons for the behaviours of surrounding vehicles, which will significantly reduce unnecessary overshoot or correction in mission plan.</p><p>In urban driving scenario, vehicle to vehicle collaboration can also improve driving safety by sharing perceived obstacle information, especially moving obstacles, for example, pedestrians and cyclers.</p><h3 id="_3-4-3-vehicle-to-infrastructure-collaboration" tabindex="-1"><a class="header-anchor" href="#_3-4-3-vehicle-to-infrastructure-collaboration"><span>3.4.3 VEHICLE TO INFRASTRUCTURE COLLABORATION</span></a></h3><ul><li>The collaboration between vehicles and infrastructures benefits urban driving.</li><li>Since infrastructures, for example, traffic lights, RADARs and cameras, are fixed over the road or along the roadside, they can have much more accurate observations of the environment than the moving vehicles.</li><li>Vehicles that may drive through a congested area can then reschedule its route in advance.</li><li>Around sharp corners where vehicle onboard sensors may have limited field of vision, cameras mounted on the roadside can help to detect children running across and prevent the happening of accidents by sending a warning to passing-by vehicles.</li><li>At intersections, the traffic light can share its state to vehicles that are planning to pass it.</li><li>The vehicles can make adjustment in their mission planner to avoid sudden brake or acceleration.</li><li>In certain area where human drivers are not presenting, the traffic lights can even be removed to save infrastructure construction cost.</li></ul><h2 id="_4-conclusions-and-future-directions" tabindex="-1"><a class="header-anchor" href="#_4-conclusions-and-future-directions"><span>4. CONCLUSIONS AND FUTURE DIRECTIONS</span></a></h2><ul><li>As a typical CPS system, autonomous vehicles incorporate the three essential factors of computing, communication and control.</li><li>Among the three factors, the real-time control system is the most sophisticated due to the involvement of various components and functions.</li><li>Based on the hardware platform constructed by ECUs, sensors and communication buses, each required function is implemented in a form of embedded software.</li><li>Instead of a large constellation of sensors working together, future autonomous vehicles will be a real intelligent system, which is the ultimate goal of the autonomous vehicle CPS.</li><li>In order to enable autonomous driving functions, the vehicle should rely on the combination of different sensors to perceive the environment with a very high precision and reliability.</li></ul><hr><ul><li>However, each sensor technology has its own shortcomings and capability limitations, e.g., some of them may be impaired in bad weather scenarios.</li><li>These shortcomings make it difficult for any of the sensors to be used as a standalone system.</li><li>Moreover, the failure of one or more sensors will possibly result in the malfunction of the autonomous vehicle control system.</li><li>One way to minimize this is to ‘fuse’ the data emanating from various sources.</li><li>This is commonly known as sensor fusion.</li><li>A fused sensor system combines the benefits of multiple sensors, i.e., RADARs, LiDARs, GPS and cameras, to construct data sources with redundancies.</li><li>In addition, to make the real-control system more robust, fault diagnosis, tolerance control and the sensorless estimation technology also need to be further researched and applied in the TCPS.</li><li>Fully autonomous vehicles and intelligent connected vehicles will be the features of future ground transportation.</li><li>Humanevehicle interaction and riding comfort will also be the goals of autonomous driving system, as well as of all the autonomous functions.</li></ul>',72)]))}const v=t(u,[["render",p],["__file","Week04.html.vue"]]),b=JSON.parse('{"path":"/306/Week04.html","title":"Chapter 4: Real-time control systems","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"UNDERSTANDING CONTROL SYSTEM","slug":"understanding-control-system","link":"#understanding-control-system","children":[]},{"level":2,"title":"OBJECTIVES","slug":"objectives","link":"#objectives","children":[{"level":3,"title":"1. INTRODUCTION","slug":"_1-introduction","link":"#_1-introduction","children":[]},{"level":3,"title":"2. COMPONENTS IN REAL-TIME CONTROL SYSTEMS:","slug":"_2-components-in-real-time-control-systems","link":"#_2-components-in-real-time-control-systems","children":[]}]},{"level":2,"title":"2.2 STRUCTURE OF THE REAL-TIME CONTROL SYSTEM OF AUTONOMOUS VEHICLES","slug":"_2-2-structure-of-the-real-time-control-system-of-autonomous-vehicles","link":"#_2-2-structure-of-the-real-time-control-system-of-autonomous-vehicles","children":[]},{"level":2,"title":"2.3 ELECTRONIC CONTROL UNITS","slug":"_2-3-electronic-control-units","link":"#_2-3-electronic-control-units","children":[]},{"level":2,"title":"2.4 SENSORS IN AUTONOMOUS VEHICLES","slug":"_2-4-sensors-in-autonomous-vehicles","link":"#_2-4-sensors-in-autonomous-vehicles","children":[{"level":3,"title":"2.4.1 ULTRASONIC SENSORS","slug":"_2-4-1-ultrasonic-sensors","link":"#_2-4-1-ultrasonic-sensors","children":[]}]},{"level":2,"title":"2.5 ACTUATORS","slug":"_2-5-actuators","link":"#_2-5-actuators","children":[]},{"level":2,"title":"3. REAL-TIME CONTROL SYSTEMS IN AUTONOMOUS VEHICLES","slug":"_3-real-time-control-systems-in-autonomous-vehicles","link":"#_3-real-time-control-systems-in-autonomous-vehicles","children":[{"level":3,"title":"3.1.2 LOCALISATION","slug":"_3-1-2-localisation","link":"#_3-1-2-localisation","children":[]},{"level":3,"title":"3.3.4 NEURAL NETWORKEBASE D ADVANCED CONTROL","slug":"_3-3-4-neural-networkebase-d-advanced-control","link":"#_3-3-4-neural-networkebase-d-advanced-control","children":[]},{"level":3,"title":"3.3.4 NEURAL NETWORK-BASED ADVANCED CONTROL","slug":"_3-3-4-neural-network-based-advanced-control","link":"#_3-3-4-neural-network-based-advanced-control","children":[]},{"level":3,"title":"3.3.6 MODEL PREDICTIVE CONTROL","slug":"_3-3-6-model-predictive-control","link":"#_3-3-6-model-predictive-control","children":[]}]},{"level":2,"title":"3.4 AUTONOMOUS VEHICLE COLLABORATION IN TRANSPORTATION CYBER-PHYSICAL SYSTEM","slug":"_3-4-autonomous-vehicle-collaboration-in-transportation-cyber-physical-system","link":"#_3-4-autonomous-vehicle-collaboration-in-transportation-cyber-physical-system","children":[{"level":3,"title":"3.4.2 VEHICLE TO VEHICLE COLLABORATION","slug":"_3-4-2-vehicle-to-vehicle-collaboration","link":"#_3-4-2-vehicle-to-vehicle-collaboration","children":[]},{"level":3,"title":"3.4.3 VEHICLE TO INFRASTRUCTURE COLLABORATION","slug":"_3-4-3-vehicle-to-infrastructure-collaboration","link":"#_3-4-3-vehicle-to-infrastructure-collaboration","children":[]}]},{"level":2,"title":"4. CONCLUSIONS AND FUTURE DIRECTIONS","slug":"_4-conclusions-and-future-directions","link":"#_4-conclusions-and-future-directions","children":[]}],"git":{"updatedTime":1728193501000,"contributors":[{"name":"rhyme_yang","email":"rhyme_yang@live.cn","commits":2}]},"filePathRelative":"306/Week04.md"}');export{v as comp,b as data};
